[["index.html", "DATA 101 Shortest Textbook How To Accomplish More With Less ", " DATA 101 Shortest Textbook How To Accomplish More With Less Tomasz Imielinski timielinski@gmail.com This is a textbook based on the DATA 101 course thought by Prof. Tomasz Imielinski at Computer Science Department at Rutgers University, New Brunswick. Data 101 is an introductory course for beginners from any field of study, interested in the field of Data Science. This books pages are created using Rmarkdown from Rstudio (similar to jupyter notebook) and the book is compiled using bookdown package. The most important aspect of this interactive book are the interactive code chunks for running code, which are powered by a minimal version of Datacamps learning interface called Datacamp Light. For more info visit Datacamp Light Note1: This book is undergoing constant updates following along with the course thought in Spring 2021. Topics under progress are marked with a \" * \". Note2: This book uses Datacamp Light for supporting runnable code chunks. In case the code chunks do not connect to run-time session, please copy the code and run in RStudio. Also, please report if facing this issue to the instructor via email. "],["intro.html", "Chapter 1 Introduction 1.1 Setting Up R", " Chapter 1 Introduction The objective of this textbook is to provide you the shortest path to exploring your data, visualizing it, forming hypotheses and validating and defending them. Given a data set, you want to be able to make any plot you wish, find plots which show something actionable and interesting, explore data by slicing and dicing it and finally present your results in statistically convincing manner, perhaps in colorful and visually appealing way. Questions which you will have to anticipate and you will have to answer are - How do you know that your findings are not random? - And fundamental of all questions: - So what? Even the most impressing looking results may come up randomly. And you will be asked this question along with the question what was your p-value and how did you compute it And even if you convince your audience that your results are not random, you will have to be ready to explain why should you audience care about the results you reported. In other words, is there any actionable value in your results? Or they are just simply interesting, good to know, but no one really needs to care much about them otherwise? Hopefully it is the former not the latter. In the following sections we will address these questions and go through the process of data exploration, validation, and presentation. We will start with making plots, follow with free style data exploration  which allows us to form the leads, that is hypotheses. Then we will follow with simple statistical tests which will allow us to validate these hypothesis and defend our findings against randomness claims. - We will learn how to calculate p-values and how to use them to defend our findings. We will use as few R commands as possible and reach our goal in shortest possible path. In fact we will demonstrate how using just 7 R commands we can perform quite sophisticated data exploration. 1.1 Setting Up R Important Instructions Installation of R is required before installing RStudio R is a programming language, and, RStudio is an Integrated Development Environment (IDE) which provides you a platform to code in R. How to download and install R &amp; RStudio? Downloading and installing R. For Windows Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cran.r-project.org/bin/windows/base/ Click on the link at top left where it says Download R 4.0.3 for windows or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. For MAC Users. Click on the link provided below or copy paste it on your favourite browser and go to the website. https://cloud.r-project.org/bin/macosx/ Under Latest release, click on R-4.0.3.pkg or the latest at the time of your installation. Open the downloaded file and follow the instructions as it is. Downloading and installing RStudio. For Windows Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named All Installers. Click on the download link beside Windows 10/8/7 to download the windows version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. For MAC Users. Click on the link below or copy paste it in your favourite browser. https://rstudio.com/products/rstudio/download/ Scroll down almost till the end of the web page until you find a section named All Installers. Click on the link beside macOS 10.13+ to start your download the MAC version of RStudio. Install RStudio by clicking on the downloaded file and following the instructions as it is. How to upload a data set? To upload the dataset/file present in csv format the read.csv() and read.csv2() functions are frequently used The read.csv() and read.csv2() have different separator symbol: for the former this is a comma, whereas the latter uses a semicolon. Let us look at the example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIFJlYWQgaW4gdGhlIGRhdGFcbmRmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L21vb2R5MjAyMGIuY3N2XCIpXG5cbiMgUHJpbnQgb3V0IGBkZmBcbmhlYWQoZGYpIn0= "],["dataexp.html", "Chapter 2 Data Exploration 2.1 Plots 2.2 Free Style data exploration with just seven R commands \" R.7 \"", " Chapter 2 Data Exploration 2.1 Plots Table 2.1: Snippet of Moody Dataset score grade texting questions participation 26.89 F never never 0.41 71.57 B always rarely 0.00 90.11 A always never 0.27 31.52 D sometimes rarely 0.68 95.94 A always rarely 0.09 45.72 D always rarely 0.19 90.82 A always always 0.25 75.52 B sometimes never 0.28 52.31 C never never 0.67 39.57 D always always 0.40 2.1.1 Scatter Plot Scatter Plot are used to plot points on the Cartesian plane (X-Y Plane) Hence it is used when both the labels are numerical values. Lets look at example of scatter plot using Moody. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExldCdzIGxvb2sgYXQgYSAyIGF0dHJpYnV0ZSBzY2F0dGVyIHBsb3QuXG4jIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5wbG90KG1vb2R5JHBhcnRpY2lwYXRpb24sbW9vZHkkc2NvcmUseWxhYj1cInNjb3JlXCIseGxhYj1cInBhcnRpY2lwYXRpb25cIixtYWluPVwiIFBhcnRpY2lwYXRpb24gdnMgU2NvcmVcIixjb2w9XCJyZWRcIikifQ== 2.1.2 Bar Plot A bar plot is used to plot rectangular bars proportional to the values present in a numerical vector. This rectangle height is proportional to the value of the variable in the vector. Barplots are also used to graphically represent the distribution of a categorical variable, after converting the categorical vector into a table(i.e. frequency distribution table) In a bar plot, you can also give different colors to each bar. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5jb2xvcnM8LSBjKCdyZWQnLCdibHVlJywnY3lhbicsJ3llbGxvdycsJ2dyZWVuJykgIyBBc3NpZ25pbmcgZGlmZmVyZW50IGNvbG9ycyB0byBiYXJzXG5cbiNsZXRzIG1ha2UgYSB0YWJsZSBmb3IgdGhlIGdyYWRlcyBvZiBzdHVkZW50cyBhbmQgY291bnRzIG9mIHN0dWRlbnRzIGZvciBlYWNoIEdyYWRlLiBcblxudDwtdGFibGUobW9vZHkkZ3JhZGUpXG5cbiNvbmNlIHdlIGhhdmUgdGhlIHRhYmxlIGxldHMgY3JlYXRlIGEgYmFycGxvdCBmb3IgaXQuXG5cbmJhcnBsb3QodCx4bGFiPVwiR3JhZGVcIix5bGFiPVwiTnVtYmVyIG9mIFN0dWRlbnRzXCIsY29sPWNvbG9ycyxcbm1haW49XCJCYXJwbG90IGZvciBzdHVkZW50IGdyYWRlIGRpc3RyaWJ1dGlvblwiLGJvcmRlcj1cImJsYWNrXCIpIn0= 2.1.3 Box Plot A boxplot shows the distribution of data in a dataset. A boxplot shows the following things: Minimum Maximum Median First quartile Third quartile Outliers You can create a single boxplot using just a vector or a multiple boxplot using a formula. When you write a formula, you should use the Tilde (~) operator. This column name on the left side of this operator goes on the y axis and the column name on the right side of this operator goes on the x axis. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5jb2xvcnM8LSBjKCdyZWQnLCdibHVlJywnY3lhbicsJ3llbGxvdycsJ2dyZWVuJykgIyBBc3NpZ25pbmcgZGlmZmVyZW50IGNvbG9ycyB0byBiYXJzXG5cblxuI1N1cHBvc2UgeW91IHdhbnQgdG8gZmluZCB0aGUgZGlzdHJpYnV0aW9uIG9mIHN0dWRlbnRzIHNjb3JlIHBlciBHcmFkZS4gV2UgdXNlIGJveCBwbG90IGZvciBnZXR0aW5nIHRoYXQuIFxuYm94cGxvdChzY29yZX5ncmFkZSxkYXRhPW1vb2R5LHhsYWI9XCJHcmFkZVwiLHlsYWI9XCJTY29yZVwiLCBtYWluPVwiQm94cGxvdCBvZiBncmFkZSB2cyBzY29yZVwiLGNvbD1jb2xvcnMsYm9yZGVyPVwiYmxhY2tcIilcblxuIyB0aGUgY2lyY2xlcyByZXByZXNlbnQgb3V0bGllcnMuIn0= 2.1.4 Mosiac Plot Mosaic plot is a graphical method for visualizing data from two or more qualitative variables. The length of the rectangles in the mosaic plot represents the frequency of that particular value. The width and length of the mosaic plot can be used to interpret the frequencies of the elements. -For example, if you want to plot the number of individuals per letter grade using a smartphone, you want to look at mosiac plot. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5jb2xvcnM8LSBjKCdyZWQnLCdibHVlJywnY3lhbicsJ3llbGxvdycsJ2dyZWVuJykgIyBBc3NpZ25pbmcgZGlmZmVyZW50IGNvbG9ycyB0byBiYXJzXG5cbiNzdXBwb3NlIHlvdSB3YW50IHRvIGZpbmQgbnVtYmVycyBvZiBzdHVkZW50cyB3aXRoIGEgcGFydGljdWxhciBncmFkZSBiYXNlZCBvbiB0aGVpciB0ZXh0aW5nIGhhYml0cy4gVXNlIE1vc2lhYy1wbG90LlxuXG5tb3NhaWNwbG90KG1vb2R5JGdyYWRlfm1vb2R5JHRleHRpbmcseGxhYiA9ICdHcmFkZScseWxhYiA9ICdUZXh0aW5nIGhhYml0JywgbWFpbiA9IFwiTW9zaWFjIG9mIGdyYWRlIHZzIHRleGluZyBoYWJpdCBpbiBjbGFzc1wiLGNvbD1jb2xvcnMsYm9yZGVyPVwiYmxhY2tcIikifQ== 2.2 Free Style data exploration with just seven R commands \" R.7 \" Now as you know to make simple, but quite colorful, basic graphs it is time to go one step further and use the plots to explore your data. This is the subject of the process known as data exploration. We will begin with what we call, the free style data exploration. We call it free style, since we are not going to use any sophisticated libraries, but rather just seven basic of commands of R. We call this set of commands - R.7. And with these seven commands of R.7 we will be able to do quite a bit of data exploration. We will be able to slice and dice data any way we want to. No statistics yet, and no more sophisticated R programs.These will come later. For now, we are just feeling the data with four plots and three R instructions: subset(), table() and tapply() Through this section we will use our usual initial example, of synthetically generate data describing mysterious methods of grading used by an eccentric Professor Moody. We have been using different versions of this data puzzle over the 6 years of teaching data 101. Data is different but narrative is always the same: 2.2.0.1 Professor Moody Puzzle Professor Moody has been teaching statistics 101 class for many years. His teaching evaluations went considerably south with the chief complaint: he DOES NOT seem to assign grades fairly. Students compared their scores among themselves and found quite a bit of discrepancies! But their complaints went nowhere since Professor promptly disappeared after posting the final grades and scores. A new brave TA, managed to get hold of the carefully maintained grading table (spanning multiple years) of professor Moody by .messing a bit with Moodys computer.well, lets not explain the details because he would get in trouble. What he found out was a remarkably structured account of how professor Moody assigns his grades. Looks like Professor Moody is in fact very alert in class. He is aware of what students do, detecting texting during class and remembering exactly who asked many questions in class. He also keeps the mysterious participation index which is a numerical score from 0 to 1. This is probably related to questions asked and answered by students as well as their general attentiveness in class. Remarkable but a little creepy, isnt it? What is the best advice the new TA, can give future students how to get a good grade in Professor Moodys class? What factors influence the grade besides the score? Back your recommendation up with plots and evidence from the attached data. The Moody data set is defined here by the following attributes Table 2.2: Snippet of Moody Dataset score grade texting questions participation 26.89 F never never 0.41 71.57 B always rarely 0.00 90.11 A always never 0.27 31.52 D sometimes rarely 0.68 95.94 A always rarely 0.09 45.72 D always rarely 0.19 90.82 A always always 0.25 75.52 B sometimes never 0.28 52.31 C never never 0.67 39.57 D always always 0.40 Moody[score, grade, participation, questions, texting] Score and grade are self explanatory. Participation is supposedly measuring students participation in class. We do not know whether higher participation would necessarily be positive, since Professor Moodys mood changes from year to year and he may be annoyed by students who are too active and bother him too much. Who knows? We have to find out. Attribute questions has several values always, frequently, sometimes and never. So does the attribute texting. In our data set there are students who are are always texting and who never ask any questions. Oh, yes, and some students participation index is almost zero. Guess what grade are they getting? F, you probably guess. Well, but about their score.It should matter probably. At least to some degree! Grading rules, which Professor Moody applies each year are different. The objective of our free style data exploration is to find some leads/hypotheses which would help us direct students what they should do to get a good grade in his class. This is a good illustration of what data exploration is and can achieve. It is just an example, but one can of course easily see that, things we discuss here, applies to any data sets. Data exploration can be viewed as an indefinite loop: REPEAT{ Plot,one or many plots. Transform Data. } UNTIL GRATIFICATION Put yourself in the position of a student in Moodys class. What does s/he want to know? What should I do in order to pass his class aside from getting the best score possible? Ask many questions? Do not text? Come to class as often as you can? Presumably improving participation index? What is the approach? First you need kick the tires, make some plots, feel the data and perhaps rule out the obvious. In case of Professor Moody data it may mean the following: - Test if straightforward mapping of scores into grades work in Professor Moodys class. Admittedly it is a long shot. We expect more from professor Moody than just merely following the scoring intervals with A above, say 85, B between 70 and 85 etc! But we need to establish that it is not the case quickly. Since it would be embarrassing to miss the obvious and simplest recommendation. Just score as high as you can. Otherwise no need to come to class, and in class you can text as much as you want to and ask no questions. Does not matter what else you do. You may never ask any questions, always text in class or simplynever even show up. All it matters is score! There is just one plot which can quickly establish whether this simple rule works. And it is the boxplot. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5ib3hwbG90KG1vb2R5JHNjb3JlIH4gbW9vZHkkZ3JhZGUsIG1haW4gPSAnRGlzdHJpYnV0aW9uIG9mIFNjb3JlcyBieSBHcmFkZScsIHlsYWIgPSdTY29yZSAob3V0IG9mIDEwMCknLCB4bGFiID0gJ0xldHRlciBHcmFkZScsY29sPWMoJ3JlZCcsJ2JsdWUnLCdncmVlbicsJ2N5YW4nLCd5ZWxsb3cnKSkifQ== As illustrated by the boxplot, there are significant overlaps between successive grades. This disproves that there is a deterministic function between score and grade. At least it is not always a function. If your score falls in certain gray areas you may get either one of two grades (A or B, B or C, C or D, D or F). And we do not know what is this additional decider in such case when score falls into this gray area. Here is how we can check which factors may impact the grade. One way of doing this analysis is to make barplots for all possible slices of Moody data frame by a given categorical variable For example,we want to know if asking questions matters for the grade? This can be validated by comparing barplots of grade distribution for different values of attribute questions.You can either do it by applying the mosaic plot which allows for two-dimensional representation of data and allows to create multicolored table for grade x questions to eyeball if values of attribute questions matter for values of attribute grade. To dig deeper into the relationship of categorical variables questions and texting with grade we will use sequence of bar plots over subsets of the Moody data frame. Then we will follow with the mosaic plots. The following slices represent subsets of the Moody data frame for each of the values of the attribute questions The command \\(\\color{violet}{\\text{table}}\\) (one of the 7 commands) will provide us grade distribution for each of these slices. And the barplot, will visualize this table. Lets look at the example of the above process for students who always ask question. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5iYXJwbG90KHRhYmxlKG1vb2R5W21vb2R5JHF1ZXN0aW9ucz09J2Fsd2F5cycsXSRncmFkZSksbWFpbiA9ICdGcmVxdWVuY3kgb2Ygc3R1ZGVudHMgYnkgR3JhZGUgd2hvIFwiYWx3YXlzXCIgYXNrIHF1ZXN0aW9ucycsIHlsYWIgPSdGcmVxdWVuY3knLCB4bGFiID0gJyBHcmFkZScsY29sPWMoJ3JlZCcsJ2JsdWUnLCdncmVlbicsJ2N5YW4nLCd5ZWxsb3cnKSlcblxuI05vdGljZSB0aGF0IHlvdSBjYW4gbW9kaWZ5IHRoZXNlIGJhcnBsb3RzIGdyYXBocyBhbmQgcmVwbGFjZSB0aGUgdmFsdWUgb2YgbW9vZHkkcXVlc3Rpb25zIGF0dHJpYnV0ZXMgZnJvbSBcImFsd2F5c1wiIHRvIFwic29tZXRpbWVzXCIgb3IgXCJuZXZlclwiIGFuZCBzZWUgaW1wYWN0IHRoZXNlIG5ldyBzbGljZXMgaGF2ZSBvbiB0aGUgZ3JhZGUgZGlzdHJpYnV0aW9uLiBKdXN0IGNoYW5nZSB0byBjb2RlIGFib3ZlIGFuZCBydW4gaXQuIFlvdSBjYW4gYWxzbyBjaGFuZ2UgdGhlIG1vb2R5JHF1ZXN0aW9ucyBhdHRyaWJ1dGUgYW5kIHJlcGxhY2UgaXQgd2l0aCB0aGUgbW9vZHkkdGV4dGluZyBhdHRyaWJ1dGUgYW5kIGl0cyBkaWZmZXJlbnQgdmFsdWVzLiBUaHVzIHlvdSBjYW4gcnVuIDYgZGlmZmVyZW50IGJhcnBsb3RzIHVzaW5nIHRoZSBjb2RlIGFib3ZlIGFuZCBzZWUgaG93IEdyYWRlIGRpc3RyaWJ1dGlvbiBpcyBhZmZlY3RlZCBmb3IgZWFjaCBvZiB0aGVzZSA2IGNhc2VzLiJ9 We can also run two mosaic plots of GRADE vs questions or texting respectively - and be able to asses the same - do these attributes matter for the grade? In the following command we can combine attribute grade with anyone of the behavioral attributes eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5tb3NhaWNwbG90KG1vb2R5JGdyYWRlfm1vb2R5JHRleHRpbmcseGxhYiA9ICdHcmFkZScseWxhYiA9ICdUZXh0aW5nIGhhYml0JywgbWFpbiA9IFwiTW9zaWFjIG9mIGdyYWRlIHZzIHRleGluZyBoYWJpdCBpbiBjbGFzc1wiLGNvbD1jKCdyZWQnLCdibHVlJywnZ3JlZW4nLCdjeWFuJywneWVsbG93JyksYm9yZGVyPVwiYmxhY2tcIikifQ== This can be concluded by comparing different columns and rows of the mosaic table. If grade distribution is similar for different values of behavioral attributes, this would indicate that these attributes do not matter in the establishing the grade. On the other hand we may catch professor Moody and find out that for some value of some attribute, grade distribution is significantly affected. This was the case several years ago when students sitting in the first row, got a grade bump up, even if they get similar scores to students sitting in the back row. In that case one of the extra attributes included the row where students were sitting during class. We can see that asking many questions (frequently and always) really matters for the grade, there is more As and more Bs for these slices than in general. But this may have nothing to do with Professor Moody rewarding students with the bonus for asking questions. It may be simply the case that such students are more involved and study harder (or are more interested in the topic) and simply get higher scores. We need to dig deeper and see which of the two is the case. Moodys just gives his personal bonus to students who ask a lot of questions or no such bonus is given  such students simply score higher. We can accomplish this using again one of the seven R commands  the tapply. ## always never rarely ## 51.08277 56.32474 53.69217 will return average score for each of the values of the attribute moody$questions. If this values are more or less uniform than it will informally (not statistically yet, for this we have to wait for the next sections) show that questions matter in professor moody grading method and are not just correlated with students score. Take a look at eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5iYXJwbG90KHRhcHBseShtb29keSRzY29yZSwgbW9vZHkkcXVlc3Rpb25zLCBtZWFuKSwgeGxhYiA9ICdxdWVzdGlvbiBjYXRlZ29yaWVzJyx5bGFiID0gJ1Njb3JlIEF2ZXJhZ2UnLCBtYWluID0gXCJNZWFuIFNjb3JlIHZzIFF1ZXN0aW9ucyBBc2tlZCB1c2luZyB0YXBwbHkoKVwiLGNvbD1jKCdyZWQnLCdibHVlJywnZ3JlZW4nLCdjeWFuJywneWVsbG93JyksYm9yZGVyPVwiYmxhY2tcIikifQ== What is the conclusion? Does asking questions often imply higher score? Or it does not affect the score but affects the grade through the grading rules of Professor Moody. Similarly, does asking questions often imply higher score? Or it does not affect the score but affects the grade through the grading rules of Professor Moody. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5iYXJwbG90KHRhcHBseShtb29keSRzY29yZSwgbW9vZHkkdGV4dGluZywgbWVhbiksIHhsYWIgPSAndGV4dGluZyBjYXRlZ29yaWVzJyx5bGFiID0gJ1Njb3JlIEF2ZXJhZ2UnLCBtYWluID0gXCJNZWFuIFNjb3JlIHZzIFRleHRpbmcgdXNpbmcgdGFwcGx5KClcIixjb2w9YygncmVkJywnYmx1ZScsJ2dyZWVuJywnY3lhbicsJ3llbGxvdycpLGJvcmRlcj1cImJsYWNrXCIpIn0= shows that mean scores are the same across different values of the textingattribute. Same is true for the mean scores of questions attribute. Neither do these two attributes texing and questions seem to have impact on the grade. Therefore it seems that the behavioral attributes: questions and texting do not seem to have an impact on the grade. Lets examine participation attribute. This is the only culprit left - to explain different grades in the grey intervals of score. We define intervals of score as clear, if there is only one grade associated with scores from such interval. The remaining intervals are defined as grey - scores where grade can be either A or B, B or C, C or D and D or F respectively. Then we can examine how participation influences grades in these gray areas of score. Our hypothesis is that higher participation would probably offer better odds for higher grade. We can run the following command for different values of q. Let q be a threshold of participation which we want to test. May be if participation is higher than q, higher grade (from the two possible grades in the gray area of score) is given, while if participation is lower than q, it works against a student, who then gets lower grade? Lets check how the grade distribution changes for different values of q from the lower values of q to higher values of q. We can just change q directly in the code below, and see results immediately. Run the following command for different values of q. We will only show it for the grey interval between A and B. The same process can be repeated for other gray areas between B and C, C and D and D and F. In fact one can modify the code below by just replacing grade A and B with B and C respectively as well as replacing the variables LowestA with Lowest B and Highest B with Highest C respectively. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiNTaW1wbGUgUiBjb21tYW5kcyB0byBnZXQgaW50ZXJ2YWwgZm9yIGVhY2ggZ3JhZGUuXG5Mb3dlc3RBPC1taW4obW9vZHlbbW9vZHkkZ3JhZGU9PSdBJywgXSRzY29yZSlcbkhpZ2hlc3RCPC1tYXgobW9vZHlbbW9vZHkkZ3JhZGU9PSdCJywgXSRzY29yZSkgXG4jVGhpcyBnaXZlcyB1cyBpbnRlcnZhbCA8SGlnaGVzdEIsIExvd2VzdEE+XG5wcmludChjKExvd2VzdEEsSGlnaGVzdEIpKVxuXG5xPTAuNSAjIFBsZWFzZSBFZGl0IHRoaXMgXCIgcSBcIiB2YWx1ZSBhbmQgc2VlIHRoZSBjaGFuZ2VzIGFzIG1lbnRpb25lZCBhYm92ZS5cbnRhYmxlKG1vb2R5W21vb2R5JHNjb3JlPkxvd2VzdEEgJiBtb29keSRzY29yZTxIaWdoZXN0QiYgbW9vZHkkcGFydGljaXBhdGlvbiA+IHEsXSRncmFkZSlcblxuI05vdGUgdGhlIHNhbWUgcHJvY2VzcyBjYW4gYmUgcmVwZWF0ZWQgd2l0aCBvdGhlciBhZGphY2VudCBncmFkZXMuIEV4OiA8QixDPiBldGMuIn0= Please verify that for higher values of 0&lt;q&lt;1, the table operation show higher percentages of better grades. Is there a critical value of q which clearly separates, say As from Bs? It seems to be q=0.6 - but it is not a clear cut deterministic. Rather, higher value of participation threshold, q increases probability (frequency) of getting As. We come to conclusion that participation matter in grey area of score, in having higher chance for better grade, if participation is higher. Thus, just in case (since no one can predict if they will end up in border line score) it is better to earn high participation index  by (probably) coming to class more often and participating in discussions, and answering professor Moodys questions. Simply put come to as many classes as you can. But while in class, do not worry about texting or asking questions. These two attributes do not seem to matter. Now we can reveal how data was generated? What was the real rule embedded in the data. Now it is time to reveal how we generated our data? This what we embedded in our data generation method: participation increases the chances of higher grade in the gray areas of score (border areas). We have indeed defined border areas in score value. In this border areas of score, participation plays a role. Student whoses score falls into the grey area may get one of two grades, A or B, B or C, C or D and D or F, depending on the score.For example score of 72 may result in A or B. It is more likely to be A if students participation is high (higher the better the odds of getting A). If students participation is low, it is much more likely to result in lower grade, for the score of 72, it would be B. Therefore we have discovered more or less the rule which guided generation of the Moodys data set and provided students with actionable intelligence how to increase chances of getting higher grade. Relationship between participation, score and grade In the process of slicing, dicing and plotting the data we would also discover other interesting relationships still using just 7 commands. Does higher participation mean higher score, in general? Meaning that coming to class is positively correlated with higher score? We can run scatter plot eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5wbG90KG1vb2R5JHBhcnRpY2lwYXRpb24sIG1vb2R5JHNjb3JlKSJ9 To our surprise it looks like the higher the participation, the lower the score! The distinct linear patterns in scatter graph seem to be sloping down with participation. We are tempted to infer that participation is bad for score, that somehow Moodys lectures have negative impact on the score - hence do not carry pedagogical value. Such conclusion is typical confusion between correlation and causation. What is true in our simulated data set - that students had some prior background in the subject matter of Professor Moody just do not show up in class that often. They already know the material. Students who do show up are the ones who are not confident in their knowledge of the subject matter - in general weak As and below. Then of course lower grade students (Ds and Fs) just simply do not apply themselves that much - are not invested into the class and just show up in class even less. Thus, the explanation probably has more to do with the students attribute about the class than with the pedagogical value of Professor Moodys lectures. We can change values of parameters q and s and examine in more detail the relationships between scores and participation. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiNJbnRlcmVzaW5nIGFuYWx5c2lzIGhhcyB0byBkbyB3aXRoIGRpZ2dpbmcgZGVlcGVyIGludG8gcmVsYXRpb25zaGlwIGJldHdlZW4gcGFydGljcGF0aW9uIGFuZCBzY29yZS4gV2hhdCBhcmUgdGhlIHNjb3JlcyBvZiBzdHVkZW50cyB3aG8gcGFydGljaXBhdGUgbGVzcyB0aGFuIHNvbWUgdmFsdWUgcSA8MS4gV2hhdCBhcmUgdGhlIHBhcnRpY2lwYXRpb24gdmFsdWVzIG9mIHN0dWRlbmV0cyB3aG8gc2NvcmUgbGVzcyB0aGFuIHM/ICAgQnkgY2hhbmdpbmcgdGhlIHZhbHVlcyBvZiBxIGFuZCBzIGluIHRoZSBjb2RlIHlvdSBjYW4gZ2FpbiBtb3JlIGluc2lnaHQgaW50byByZWxhdGlvbnNoaXAgYmV0d2VlbiBwYXJ0aXBhdGlvbiBhbmQgc2NvcmUuXG5cbiMgQ2hhbmdlIHRoZSB2YWx1ZXMgb2YgXCJxXCIgYW5kIFwic1wiIGJlbG93LlxucTwtMC4xXG5zPC03MFxubWVhbihtb29keVttb29keSRwYXJ0aWNpcGF0aW9uIDxxLF0kc2NvcmUpXG5tZWFuKG1vb2R5W21vb2R5JHNjb3JlIDxzLF0kcGFydGljaXBhdGlvbikifQ== Exploring Behaviors of Students in professor Moodys class. One may even drop the grade entirely from the picture and simply inquire about behavioral characteristics of Professor Moodys students. We already know what is the distribution of each type of behavior eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG50YWJsZShtb29keSRxdWVzdGlvbnMpO1xudGFibGUobW9vZHkkdGV4dGluZykifQ== But lets ask for associations between behaviors Do students who ask a lot of questions also spend little time texting? Do students who participate more, generally texting less? These questions have nothing to do with students performance. But can all be answered using simple R.7 commands. Same data may serve different purposes. We started with predicting what behaviors help getting higher grade in professor Moodys class. But we can imagine a different study  which is addressing student behavior in professor Moodys class. Yet another study could address the impact of behavioral attributes on students scores (not grades). All these analysis can be done using or free style exploration and R.7. What we discover, and lets be very clear about it is not yet guaranteed to be statistically valid. For this we need statistical evaluation, The p-values, the z-tests etc. Later we will also find statistical functions which can greatly help in data exploration. Free style exploration role is to generate leads known otherwise as conjectures or hypotheses. Our professor Moodys data puzzle has been traditional the first data puzzle we ask students to solve in data 101 class. Here are some examples of conclusions reached on different instances of professor Moodys data set in the past semesters. Notice that attributes of Moodys data set in past may have been different (like sleeping in class) Here is the set of recommendations from former student who cracked that years professor Moodys puzzle (or did she?) Judging by plots and means calculated earlier, there are several factors, besides score, that affect students grades:  Sleeping in class increases grade  Texting in class decreases grades a little  Being active(participating) in class all the time significantly increases the grade, BUT:  Being active(participating) in class just occasionally decreases the grade even more, than not participating at all.  Being active only occasionally significantly decreases the grades.  Texting does not significantly affect grades . So for students in order to succeed in professor Moodys class, my advice will be(besides getting high score):  VERY IMPORTANT: Participate all the time., or do not participate at all!!!  Sleep in class(especially if you do not participate anyway)  While texting might bring down your grade a little bit, the difference is very small "],["stateval.html", "Chapter 3 Simple Statistical Evaluation 3.1 *z-test 3.2 Permutation Test 3.3 *Multiple Hypothesis - Bonferroni Correction.", " Chapter 3 Simple Statistical Evaluation The biggest enemy of your findings is randomness. In order to convince your audience that you have found something you need to address the question how do you know your result is simply sheer luck, it is random? This is where you need statistics and simple hypothesis testing. 3.0.0.1 *Some Basics First: Mean \\[\\begin{equation} \\bar{X}=\\frac{\\sum{X}}{N} \\ \\text{where, X is set of numbers and N is size of set.} \\end{equation}\\] Standard Deviation \\[\\begin{equation} \\sigma = \\sqrt{\\frac{\\sum{(X - \\mu)^2}}{N}}\\\\ \\text{where, X is set of numbers, $\\mu$ is average of set of numbers, }\\\\ \\text{ N is size of the set, $\\sigma$ is standard deviation} \\end{equation}\\] 3.1 *z-test A z-test is any statistical test used in hypothesis testing with an underlying normal distribution. In other words, when the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution, z-test can be used. Outcome of the z-test is the z-score which is a numerical measure to test the mean of a distribution. z-score is measured in terms of standard deviation from mean. INCLUDE EXAMPLES 3.2 Permutation Test Permutation test allows us to observe randomness directly, with naked eye, without the lenses of statistical tests such as z-tests etc. We shuffle data randomly like a deck of cards. There may be many such shuffles - 10,000, 100,000 etc. The goal is always to see how often we can obtained the observed difference of means (since we are testing either one sided or two sided hypothesis), by purely random shuffles of our data. These permutations (shuffles) destroy all relationships which may pre-exist in our data. We are hoping to show that our observed difference of means can be obtained very rarely in completely random fashion. Then we experimentally show that our result is unlikely to randomly occur under null hypothesis. Then we can reject the null hypothesis. The less often our result appear in the histogram of permutation test results, the better the news for our alternative hypothesis. What is surprising to many newcomers, is that permutation test will give different p-values (not dramatically different, but still different) in each run of permutation test. This is the case because permutation test in random itself. It is not like z-test which will give the same result when run again for the same hypothesis and same data set. Also p-value computed by permutation test will be, in general, different than p-value computed by z-test. Not very different but different. Again, it is the case because permutation test provides only approximation of p-value. Great advantage of permutation test is that it is universal and robust. One can test different relationships between two variables than just difference of means. For example we can use permutation test to validate whether traffic in Lincoln tunnel is more than twice the traffic in Holland tunnel or even provide different weights for different days of the week. 3.2.1 Permutation Test One Step Permutation test in one step is the most direct way to see randomness close by. One step permutation function shows one single data shuffle. By shuffling the data one destroys associations which exist between values of the data frame. This make data frame random. You can execute the one step permutation multiple times. This will show how data frame varies and how does it affect the observed difference of means. Apply one step permutation function first, multiple times before you move to the proper Permutation test function. One of the parameters of the Permutation test function specifies the number of shuffles which will be preformed. This could be a very large number, 10,000 or even 100,000. The purpose of making so many random permutations is to test how often observed difference of means can arise in just random data. The more often this takes place, the more likely you observation is just random. To reject the null hypothesis you need to show that the observed difference of means will come very infrequently in permutation test. Less than 5% of the time, to be exact. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6InRyYWZmaWM8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20va3VuYWwwODk1L1JEYXRhc2V0cy9tYXN0ZXIvVFJBRkZJQy5jc3YnKSIsInNhbXBsZSI6InN1bW1hcnkodHJhZmZpYylcbkQ8LSBtZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdIb2xsYW5kJywzXSkgLSBtZWFuKHRyYWZmaWNbdHJhZmZpYyRUVU5ORUw9PSdMaW5jb2xuJywzXSlcbm51bGxfdHVubmVsIDwtIHJlcChcIkhvbGxhbmRcIiwyODAxKSAjIENyZWF0ZSAyODAxIGNvcGllcyBvZiBIb2xsYW5kIFxubnVsbF90dW5uZWxbc2FtcGxlKDI4MDEsMTQwMCldIDwtIFwiTGluY29sblwiICMgUmVwbGFjZSBSQU5ET01MWSAxNDAwIGNvcGllcyB3aXRoIExpbmNvbG5cbm51bGwgPC0gZGF0YS5mcmFtZShudWxsX3R1bm5lbCx0cmFmZmljWywzXSlcbm5hbWVzKG51bGwpIDwtIGMoXCJUVU5ORUxcIixcIlZPTFVNRV9QRVJfTUlOVVRFXCIpXG5zdW1tYXJ5KG51bGwpXG5ob2xsYW5kX251bGwgPC0gbnVsbFtudWxsJFRVTk5FTCA9PSBcIkhvbGxhbmRcIiwyXVxubGluY29sbl9udWxsIDwtIG51bGxbbnVsbCRUVU5ORUwgPT0gXCJMaW5jb2xuXCIsMl1cbm1lYW4oaG9sbGFuZF9udWxsKVxubWVhbihsaW5jb2xuX251bGwpXG5EX251bGwgPC0gbWVhbihsaW5jb2xuX251bGwpIC0gbWVhbihob2xsYW5kX251bGwpXG5jYXQoXCJUaGUgbWVhbiBkaWZmZXJlbmNlIG9mIHBlcm11dGF0aW9uIG9uZSBzdGVwIGRhdGE6IFwiLCBEX251bGwsXCJcXG5cIikjIENhbGN1bGF0ZSB0aGUgZGlmZmVyZW5jZSBiZXR3ZWVuIHRoZSBtZWFuIG9mIHRoZSByYW5kb20gZGF0YS5cbmNhdChcIlRoZSBtZWFuIGRpZmZlcmVuY2Ugb2Ygb3JpZ2luYWwgZGF0YTogXCIsIEQpICMgRGlmZmVyZW5jZSBvZiBtZWFuIHZhbHVlIG9mIG9yaWdpbmFsIGRhdGEuIn0= 3.2.2 Permutation Function The permutation function is used to run multiple iteration of the one-step permutation studied above, to get a complete relational understanding between the components involved in any hypothesis. Here you can run the example of running the permutation test on the Traffic.csv dataset, on volume of traffic in Holland and Lincoln Tunnel. eyJsYW5ndWFnZSI6InIiLCJwcmVfZXhlcmNpc2VfY29kZSI6InRyYWZmaWM8LXJlYWQuY3N2KCdodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20va3VuYWwwODk1L1JEYXRhc2V0cy9tYXN0ZXIvVFJBRkZJQy5jc3YnKVxuUGVybXV0YXRpb24gPC0gZnVuY3Rpb24oZGYxLGMxLGMyLG4sdzEsdzIpe1xuICBkZiA8LSBhcy5kYXRhLmZyYW1lKGRmMSlcbiAgRF9udWxsPC1jKClcbiAgVjE8LWRmWyxjMV1cbiAgVjI8LWRmWyxjMl1cbiAgc3ViLnZhbHVlMSA8LSBkZltkZlssIGMxXSA9PSB3MSwgYzJdXG4gIHN1Yi52YWx1ZTIgPC0gZGZbZGZbLCBjMV0gPT0gdzIsIGMyXVxuICBEIDwtICBhYnMobWVhbihzdWIudmFsdWUyLCBuYS5ybT1UUlVFKSAtIG1lYW4oc3ViLnZhbHVlMSwgbmEucm09VFJVRSkpXG4gIG09bGVuZ3RoKFYxKVxuICBsPWxlbmd0aChWMVtWMT09dzJdKVxuICBmb3IoamogaW4gMTpuKXtcbiAgICBudWxsIDwtIHJlcCh3MSxsZW5ndGgoVjEpKVxuICAgIG51bGxbc2FtcGxlKG0sbCldIDwtIHcyXG4gICAgbmYgPC0gZGF0YS5mcmFtZShLZXk9bnVsbCwgVmFsdWU9VjIpXG4gICAgbmFtZXMobmYpIDwtIGMoXCJLZXlcIixcIlZhbHVlXCIpXG4gICAgdzFfbnVsbCA8LSBuZltuZiRLZXkgPT0gdzEsMl1cbiAgICB3Ml9udWxsIDwtIG5mW25mJEtleSA9PSB3MiwyXVxuICAgIERfbnVsbCA8LSBjKERfbnVsbCxtZWFuKHcyX251bGwsIG5hLnJtPVRSVUUpIC0gbWVhbih3MV9udWxsLCBuYS5ybT1UUlVFKSlcbiAgfVxuICBteWhpc3Q8LWhpc3QoRF9udWxsLCBwcm9iPVRSVUUpXG4gIG11bHRpcGxpZXIgPC0gbXloaXN0JGNvdW50cyAvIG15aGlzdCRkZW5zaXR5XG4gIG15ZGVuc2l0eSA8LSBkZW5zaXR5KERfbnVsbCwgYWRqdXN0PTIpXG4gIG15ZGVuc2l0eSR5IDwtIG15ZGVuc2l0eSR5ICogbXVsdGlwbGllclsxXVxuICBwbG90KG15aGlzdClcbiAgbGluZXMobXlkZW5zaXR5LCBjb2w9J2JsdWUnKVxuICBhYmxpbmUodj1ELCBjb2w9J3JlZCcpXG4gIE08LW1lYW4oRF9udWxsPkQpXG4gIHJldHVybihNKVxufSIsInNhbXBsZSI6IlBlcm11dGF0aW9uKHRyYWZmaWMsIFwiVFVOTkVMXCIsIFwiVk9MVU1FX1BFUl9NSU5VVEVcIiwxMDAwLFwiSG9sbGFuZFwiLCBcIkxpbmNvbG5cIikifQ== Note: You can find the permutation function code here: Permutation() 3.2.3 Exercise - How p-value is affected by difference of means and standard deviations Here, you can generate your own data by changing parameters of the rnorm() function. See how changing the mean and sd in rnorm distributions affects the p-value! Again you can do it directly in the code and observe the results immediately. It is very revealing.. Think of Val1, and Val2 as traffic volumes in Holland and Lincoln tunnels respectively. The larger the difference between the means of rnorm() function the smaller the p-value - since it is less and less likely that observed difference of means would come frequently, due to random shuffles of permutation function. Now keep the same means and change the variances. See how changing the variances in rnorm() will affect the p-value and try to explain the effect that standard deviations have on the p-value. In general, the higher the standard deviation, the more widely data is centered around the mean. Thus even for the same two means, and two different value of deviations, we can see larger value of deviation to lead to higher p-value. Since we are less certain of the role of the mean if standard deviation is higher. Therefore, the chance of randomly obtaining the observed result, is higher. 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 3.3 *Multiple Hypothesis - Bonferroni Correction. "],["revision.html", "Chapter 4 Revision of R commands 4.1 c() &amp; data.frame() &amp; class() 4.2 summary(), mean(),length(), max(),min(), sd(),nrow(), ncol(), dim() 4.3 Table 4.4 Subset 4.5 tapply 4.6 Cut 4.7 What would R say?", " Chapter 4 Revision of R commands In this chapter we are going to recap at some basic and useful functions we have used in R. The examples we use here will be helpful in revising few of the functions we studied and would give an baseline of function we would need in the future while programming in R. List of commands: c() data.frame() subset() table() tapply() cut() summary(), mean(),length(), max(),min(), sd(),nrow(), ncol() class() And also all the plot commands present in section 2.1 4.1 c() &amp; data.frame() &amp; class() c() The c() function is used for combining arguments. The default behavior of the c() method is to combine its arguments to form a vector. All arguments are coerced (forcibly converted) to a common type which is the type of the returned value. For example,the non-character values are coerced to character type if one of the elements is a character. the hierarchy followed is NULL &lt; raw &lt; logical &lt; integer &lt; double &lt; complex &lt; character &lt; list &lt; expression. dataframe() A data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column. Following are the characteristics of a data frame. The column names should be non-empty. The row names should be unique. The data stored in a data frame can be of numeric, factor or character type. Each column should contain same number of data items. class() The class() function has multiple uses, but for here, it is used to check the type of object. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBjcmVhdGUgMyB2ZWN0b3JzIHdpdGggdGl0bGUsIGF1dGhvciBhbmQgeWVhci5cbnRpdGxlIDwtIGMoJ0RhdGEgU21hcnQnLCdPcmllbnRhbGlzbScsJ0ZhbHNlIEltcHJlc3Npb25zJywnTWFraW5nIFNvZnR3YXJlJylcbmF1dGhvciA8LSBjKCdGb3JlbWFuLCBKb2huJywnU2FpZCwgRWR3YXJkJywnQXJjaGVyLCBKZWZmZXJ5JywnT3JhbSwgQW5keScpXG55ZWFyIDwtIGMoMjAxMCwyMDExLDIwMTIsMTk5OClcblxuI0xldHMgbG9vayBhdCBob3cgdGhlIGNyZWF0ZWQgdmVjdG9ycyBsb29rLlxudGl0bGVcbmF1dGhvclxueWVhclxuXG4jIEFsc28gbGV0cyBsb29rIGF0IHRoZWlyIHR5cGVzIHVzaW5nIHRoZSBjbGFzcyBmdW5jdGlvbi5cbmNsYXNzKHRpdGxlKVxuY2xhc3MoYXV0aG9yKVxuY2xhc3MoeWVhcilcblxuXG4jIE5vdyBsZXRzIGNyZWF0ZSBhIGRhdGFmcmFtZSB1c2luZyB0aGUgYWJvdmUgY29sdW1uIHZlY3RvcnMuXG5cbmRmIDwtIGRhdGEuZnJhbWUodGl0bGUsIGF1dGhvciwgeWVhcilcbmRmICMgTGV0cyBsb29rIGF0IGhvdyB0aGUgZGF0YWZyYW1lIGxvb2tzLiJ9 4.2 summary(), mean(),length(), max(),min(), sd(),nrow(), ncol(), dim() The functions in this section are very simple yet are always useful to get more information from data. summary() function computes summary statistics of data. mean() function is used to find the average of the data. sd() fucntion is used to find the standard deviation of the data. length() function is used to get or set the length of data. max() function is used to get the maximum valued element in the data. min() function is used to get the minimum valued element in the data. nrow() function is used to find the number/count of the rows present in data. ncol() function is used to find the number/count of the columns present in data. dim() function is used to find the dimensions of the data. Lets look at example of all these functions. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBMZXRzIGxvb2sgYXQgdGhlIHN1bW1hcnlcbnN1bW1hcnkobW9vZHkpXG5cbiNMZXRzIGxvb2sgYXQgdGhlIG51bWJlciBvZiByb3dzIGluIHRoZSBkYXRhc2V0LlxubnJvdyhtb29keSlcblxuI0xldHMgbG9vayBhdCB0aGUgbnVtYmVyIG9mIGNvbHVtbnMgaW4gdGhlIGRhdGFzZXQuXG5uY29sKG1vb2R5KVxuXG4jTGV0cyBsb29rIGF0IHRoZSBkaW1lbnNpb25zIGkuZS4gYm90aCBudW1iZXJzIG9mIHJvd3MgYW5kIGNvbHVtbnMgb2YgdGhlIGRhdGEgdXNpbmcganVzdCBvbmUgY29tbWFuZFxuZGltKG1vb2R5KVxuXG4jTGV0cyBsb29rIGF0IHRoZSBtZWFuIG9mIHNjb3JlIGNvbHVtbi5cbm1lYW4obW9vZHkkc2NvcmUpXG5cbiNMZXRzIGxvb2sgYXQgdGhlIHN0YW5kYXJkIGRldmlhdGlvbiBvZiBzY29yZSBjb2x1bW5cbnNkKG1vb2R5JHNjb3JlKVxuXG4jTGV0cyBsb29rIGF0IHRoZSBsZW5ndGggb2YgdGhlIGdyYWRlIGNvbHVtbiBcbmxlbmd0aChtb29keSRncmFkZSlcblxuI0xldHMgbG9vayBhdCB0aGUgbWluaW11bSB2YWx1ZSBvZiBzY29yZSBpbiB0aGUgc2NvcmUgY29sdW1uLlxubWluKG1vb2R5JHNjb3JlKVxuXG4jbGV0cyBsb29rIGF0IHRoZSBtYXhpbXVtIHZhbHVlIG9mIHRoZSBzY29yZSBpbiB0aGUgc2NvcmUgY29sdW1uXG5tYXgobW9vZHkkc2NvcmUpIn0= 4.3 Table table() function in R Language is used to create a categorical representation of data with variable name and the frequency in the form of a table. More use of table() is when you use multiple categorical columns. For example, well see the count of grade vs asks_questions in example 2. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuXG50YWJsZWV4MTwtIHRhYmxlKG1vb2R5JEdSQURFKSAjVXNlIG9mIHRhYmxlICBmdW5jdGlvbiBvbiB0aGUgbmV3IGNvbHVtbi5cbnRhYmxlZXgxXG5iYXJwbG90KHRhYmxlZXgxLGNvbCA9YyhcInJlZFwiLFwicHVycGxlXCIsXCJjeWFuXCIsXCJ5ZWxsb3dcIixcImdyZWVuXCIpLHhsYWIgPSBcIkxhYmVsc1wiLCB5bGFiID0gXCJGcmVxdWVuY3lcIixtYWluID0gXCJ0YWJsZSgpIGV4YW1wbGUgMVwiKSAjcGxvdC5cblxuXG50YWJsZWV4MjwtdGFibGUobW9vZHkkR1JBREUsbW9vZHkkQVNLU19RVUVTVElPTlMpXG50YWJsZWV4MlxubW9zYWljcGxvdCh0YWJsZWV4Mixjb2wgPWMoXCJyZWRcIixcInB1cnBsZVwiLFwiY3lhblwiLFwieWVsbG93XCIsXCJncmVlblwiKSxtYWluID0gXCJ0YWJsZSgpIGV4YW1wbGUgMlwiKSJ9 4.3.1 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxudGFibGUobW9vZHlbbW9vZHkkQVNLU19RVUVTVElPTlMhPSdhbHdheXMnLF0kR1JBREUpXG4jV2hhdCB3aWxsIFIgc2F5P1xuXG4jIEEuIGVycm9yXG4jIEIuIGRpc3RyaWJ1dGlvbiBvZiBncmFkZXMgZm9yIHN0dWRlbnRzIHdobyBhbHdheXMgYXNrIHF1ZXN0aW9uc1xuIyBDLiBkaXN0cmlidXRpb24gb2YgZ3JhZGVzIGZvciBzdHVkZW50cyB3aG8gZG8gbm90IGFsd2F5cyBhc2sgcXVlc3Rpb25zICJ9 4.3.2 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxudGFibGUobW9vZHlbbW9vZHkkQVNLU19RVUVTVElPTlM9PWxpc3QoJ2Fsd2F5cycsJ25ldmVyJyksXSRHUkFERSlcbiNXaGF0IHdpbGwgUiBzYXk/XG5cbiMgQS4gZXJyb3IuXG4jIEIuIGRpc3RyaWJ1dGlvbiBvZiBncmFkZXMgZm9yIHN0dWRlbnRzIHdobyBhbHdheXMgb3IgbmV2ZXIgYXNrIHF1ZXN0aW9ucy4gIFxuIyBDLiBkaXN0cmlidXRpb24gb2YgZ3JhZGVzIGZvciBzdHVkZW50cyB3aG8gZG8gbm90IGFzayBxdWVzdGlvbnMgYWx3YXlzIG9yIG5ldmVyLiAifQ== 4.4 Subset subset() function in R programming is used to create a subset of vectors, matrices or data frames based on the conditions provided in the parameters. NOTE: To create a subset, not only can you use the subset() function, but also: You can use [ ] operator. Ex: dataFrameName[columnName] Even $ operator is a subset operator. Ex: dataFrameName$columnName Also, subsetting in R (commonly called subscripting) is done with square brackets. When subscripting a data frame there will be two places inside the square brackets separated by a comma. The first part inside the square brackets corresponds to rows. The second part corresponds to columns. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuXG4jU3Vic2V0IG9mIHJvd3Ncbm1vb2R5X25ldmVyX3NtYXJ0cGhvbmU8LXN1YnNldChtb29keSxPTl9TTUFSVFBIT05FPT1cIm5ldmVyXCIpXG5ucm93KG1vb2R5KVxubnJvdyhtb29keV9uZXZlcl9zbWFydHBob25lKVxudGFibGUobW9vZHlfbmV2ZXJfc21hcnRwaG9uZSRPTl9TTUFSVFBIT05FKSAjIFlvdSBjYW4gc2VlIG9ubHkgc3R1ZGVudCBuZXZlciBvbiBzbWFydHBob25lIGFyZSBpbiB0aGUgc3Vic2V0LlxuXG4jQWx0ZXJuYXRlIHdheSB0byBzdWJzZXQuXG5tb29keV9uZXZlcl9zbWFydHBob25lX2FsdDwtbW9vZHlbbW9vZHkkT05fU01BUlRQSE9ORT09XCJuZXZlclwiLCBdXG50YWJsZShtb29keV9uZXZlcl9zbWFydHBob25lX2FsdCRPTl9TTUFSVFBIT05FKSAjIFlvdSBjYW4gc2VlIGEgc2ltaWxhciB0YWJsZSBhcyBhYm92ZS5cblxuXG4jc3Vic2V0IG9mIGNvbHVtbnNcbm1vb2R5X2V4Y2VwdDg8LXN1YnNldChtb29keSwgc2VsZWN0ID0gLWMoOCkpXG5uY29sKG1vb2R5KVxubmNvbChtb29keV9leGNlcHQ4KSAjIFlvdSBjYW4gc2VlIHRoZSBudW1iZXIgb2YgY29sdW1ucyBoYXMgYmVlbiByZWR1Y2VkIGJ5IDEsIGR1ZSB0byBzdWJzZXR0aW5nIHdpdGhvdXQgY29sdW1uIDhcblxuI1N1YnNldCBvZiBSb3dzIGFuZCBDb2x1bW5zXG5tb29keV9leGNlcHQ4X25ldmVyPC1zdWJzZXQobW9vZHksIHNlbGVjdCA9IC1jKDgpLCBPTl9TTUFSVFBIT05FID09IFwibmV2ZXJcIilcbnRhYmxlKG1vb2R5X2V4Y2VwdDhfbmV2ZXIkT05fU01BUlRQSE9ORSlcbmRpbShtb29keSlcbmRpbShtb29keV9leGNlcHQ4X25ldmVyKSMgWW91IGNhbiBzZWUgb25seSBzdHVkZW50IG5ldmVyIG9uIHNtYXJ0cGhvbmVzIHdpdGhvdXQgY29sdW1uIDggZGF0YSBhcmUgcHJlc2VudCBpbiB0aGUgc3Vic2V0LiJ9 4.4.1 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxubW9vZHlbbW9vZHkkU0NPUkU+PTkwLDNdXG4jIFdoYXQgd2lsbCBSIHNheT9cblxuXG4jIEEuIEdldCBzdWJzZXQgb2YgYWxsIGNvbHVtbnMgd2hpY2ggY29udGFpbnMgc3R1ZGVudHMgd2hvIHNjb3JlZCBtb3JlIHRoYW4gZXF1YWwgdG8gOTBcbiMgQi4gZXJyb3JcbiMgQy4gZ2V0IGFsbCBzY29yZSB2YWx1ZXMgd2hpY2ggYXJlIG1vcmUgdGhhbiBlcXVhbCB0byA5MFxuIyBELiBnZXQgc3Vic2V0IG9mIG9ubHkgdGhlIGdyYWRlcyBvZiBzdHVkZW50cyB3aXRoIHNjb3JlIGdyZWF0ZXIgdGhhbiBlcXVhbCB0byA5MCJ9 4.4.2 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxubW9vZHlbbW9vZHkkU0NPUkU+PTgwLjAgJiBtb29keSRHUkFERSA9PSdCJyxdIFxuIyBXaGF0IHdpbGwgUiBzYXk/XG5cbiMgQS4gc3Vic2V0IG9mIG1vb2R5IGRhdGEgZnJhbWUgd2hvIGdvdCBCIGdyYWRlLlxuIyBCLiBlcnJvci5cbiMgQy4gc3Vic2V0IG9mIG1vb2R5IGRhdGEgZnJhbWUgd2l0aCBzY29yZSBncmVhdGVyIHRoYW4gODAuXG4jIEQuIHN1YnNldCBvZiBtb29keSBkYXRhIGZyYW1lIHdpdGggc2NvcmUgbW9yZSB0aGFuIDgwIGFuZCBnb3QgQiBncmFkZS4ifQ== 4.5 tapply tapply() function in R Language is used to apply a function over a subset of vectors given by a combination of factors This is a very versatile function, as well see from the use case. Note : There are different aggregate functions that can be used. For example, Mean, Median, Variance, Sum etc. We can also factor it on multiple attributes. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuXG4jIFRvIGFwcGx5IHRhcHBseSgpIG9uIFNDT1JFIGZhY3RvcmVkIG9uIE9OX1NNQVJUUEhPTkVcblxubW9vZHlfc2NvcmVhdmc8LXRhcHBseShtb29keSRTQ09SRSxtb29keSRPTl9TTUFSVFBIT05FLG1lYW4pXG5tb29keV9zY29yZWF2ZyAjIFdlIGNhbiBzZWUgaXQgY2FsY3VsYXRlZCBtZWFuIHZhbHVlIG9mIHRoZSBzY29yZSBieSBzdHVkZW50cyB3aXRoIHJlc3BlY3QgdG8gdGhlaXIgdXNlIG9mIHBob25lIGluIGNsYXNzLlxuXG5iYXJwbG90KG1vb2R5X3Njb3JlYXZnLGNvbCA9IFwiY3lhblwiLHhsYWIgPSBcIkxhYmVsc1wiLCB5bGFiID0gXCJtZWFuX3ZhbFwiLG1haW4gPSBcInRhcHBseSgpIGV4YW1wbGUgMVwiLGxhcyA9IDIsIGNleC5uYW1lcyA9IDAuNzUpI3Bsb3RcblxuI0xldHMgZmFjdG9yIHRoZSBncmFkZXMgb24gb25fc21hcnRwaG9uZSBhcyB3ZWxsIGFzIGdyYWRlIGNhdGVnb3J5LlxuXG5tb29keS5zY29yZWF2ZzJkPC10YXBwbHkobW9vZHkkR1JBREUsbGlzdChtb29keSRPTl9TTUFSVFBIT05FLG1vb2R5JEdSQURFKSxsZW5ndGgpXG5tb29keS5zY29yZWF2ZzJkW2lzLm5hKG1vb2R5LnNjb3JlYXZnMmQpXTwtMFxubW9vZHkuc2NvcmVhdmcyZCMgV2UgY2FuIHNlZSBpdCBjYWxjdWxhdGVkIGNvdW50IG9mIHRoZSBncmFkZSBvZiBzdHVkZW50IHdpdGggcmVzcGVjdCB0byB0aGVpciBpbi1jbGFzcyBzbWFydHBob25lIHVzYWdlICBhbmQgZ3JhZGUgY2F0ZWdvcnkuXG5iYXJwbG90KG1vb2R5LnNjb3JlYXZnMmQsY29sPWMoXCJyZWRcIixcImN5YW5cIixcIm9yYW5nZVwiLFwiYmx1ZVwiKSxtYWluID0gXCJ0YXBwbHkoKSBleGFtcGxlIDJcIixiZXNpZGUgPSBUUlVFLGxlZ2VuZD1yb3duYW1lcyhtb29keS5zY29yZWF2ZzJkKSkifQ== 4.5.1 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxudGFwcGx5KG1vb2R5LCBHUkFERSwgU0NPUkUsIG1pbilcbiMgV2hhdCB3aWxsIFIgc2F5P1xuXG4jIEEuIG1pbmltdW0gc2NvcmUgZm9yIGVhY2ggZ3JhZGVcbiMgQi4gbWluaW11bSBncmFkZSBmb3IgZWFjaCBzY29yZVxuIyBDLiBtaW5pbXVtIGdyYWRlIG9ubHkgXG4jIEQuIEVycm9yLiJ9 4.5.2 Question What would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxudGFwcGx5KG1vb2R5JEFTS19RVUVTVElPTlMsIG1vb2R5JEdSQURFLCBtZWFuKVxuIyBXaGF0IHdpbGwgUiBzYXk/XG5cbiMgQS4gbWVhbiBncmFkZSBmb3IgZWFjaCB2YWx1ZXMgb2YgYXNrX3F1ZXN0aW9uIGF0dHJpYnV0ZVxuIyBCLiBtZWFuIHZhbHVlIG9mIGFza19xdWVzdGlvbnMgYXR0cmlidXRlIGZvciBlYWNoIGdyYWRlXG4jIEMuIG1lYW4gY2F0ZWdvcnkgb2YgYXNrX3F1ZXN0aW9ucyBvbmx5IFxuIyBELiBlcnJvci4ifQ== 4.6 Cut cut() function in R Language is used to divide a numeric vector into different ranges eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuXG4jIFdlIGFjY2VzcyB0aGUgU2NvcmUgY29sdW1uIGZyb20gbW9vZHkgZGF0YXNldC5cbnNjb3JlMCA8LSBjdXQobW9vZHkkU0NPUkUsMTApXG50YWJsZShzY29yZTApICNsZXRzIGNoZWNrIHRoZSBkaXN0cmlidXRpb24gb2YgcGVvcGxlIGluIGVhY2ggcGFydGl0aW9uLlxuXG4jIEN1dCBFeGFtcGxlIHVzaW5nIGJyZWFrcyAtIEN1dHRpbmcgZGF0YSB1c2luZyBkZWZpbmVkIHZlY3Rvci4gXG5zY29yZTEgPC0gY3V0KG1vb2R5JFNDT1JFLGJyZWFrcz1jKDAsNTAsMTAwKSxsYWJlbHM9YyhcIkZcIixcIlBcIikpXG50YWJsZShzY29yZTEpIn0= 4.6.1 QuestionWhat would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxuY3V0KG1vb2R5JFNDT1JFLCBicmVha3M9YygwLDI1LDcwLDEwMCksbGFiZWxzPWMoXCJsb3dcIiwgXCJtZWRpdW1cIiwgXCJoaWdoXCIpKVxuI1doYXQgd291bGQgUiBzYXk/XG5cbiMgQS4gNSBpbnRlcnZhbHMgb2YgYXR0cmlidXRlIHNjb3JlXG4jIEIuIDMgaW50ZXJ2YWxzICgwLDI1KSAoMjUsNzApICg3NSwxMDApXG4jIEMuIDMgY2F0ZWdvcmljYWwgdmFsdWVzIFwibG93XCIsIFwibWVkaXVtXCIgYW5kIFwiaGlnaFwiIGZvciBkaWZmZXJlbnQgc2NvcmUgaW50ZXJ2YWxzXG4jIEQuIDMgc2VwYXJhdGUgZGF0YXNldHMgd2l0aCBzaW1pbGFyIHNjb3JlIHZhbHVlcyJ9 4.6.2 QuestionWhat would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxub3V0cHV0PC1jdXQobW9vZHkkU0NPUkUsIDUpXG5zdW1tYXJ5KG91dHB1dClcbiNXaGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEuIDUgaW50ZXJ2YWxzIG9mIGF0dHJpYnV0ZSBzY29yZSBvZiB1bmVxdWFsIGNvdW50IG9mIGVsZW1lbnRzXG4jIEIuIDUgaW50ZXJ2YWxzIG9mIGF0dHJpYnV0ZSBzY29yZSBvZiBlcXVhbCBjb3VudCBvZiBlbGVtZW50c1xuIyBDLiA1IGNhdGVnb3JpY2FsIHZhbHVlcyBmb3IgZGlmZmVyZW50IHNjb3JlIGludGVydmFsc1xuIyBELiA1IHNlcGFyYXRlIGRhdGFzZXQgd2l0aCBzaW1pbGFyIHNjb3JlIHZhbHVlcyJ9 4.6.3 QuestionWhat would R say? eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMV90ZXN0L21haW4vTU9PRFktMjAxOS5jc3ZcIilcblxub3V0cHV0PC1jdXQobW9vZHkkQVNLU19RVUVTVElPTlMsIDIpXG5zdW1tYXJ5KG91dHB1dClcbiNXaGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEuIDIgaW50ZXJ2YWxzIG9mIGF0dHJpYnV0ZSBhc2tfcXVlc3Rpb25zIG9mIHVuZXF1YWwgY291bnQgb2YgZWxlbWVudHMgaW4gZWFjaCBpbnRlcnZhbFxuIyBCLiAyIGludGVydmFscyBvZiBhdHRyaWJ1dGUgYXNrX3F1ZXN0aW9ucyBvZiBlcXVhbCBjb3VudCBvZiBlbGVtZW50cyBpbiBlYWNoIGludGVydmFsXG4jIEMuIDIgY2F0ZWdvcmljYWwgdmFsdWVzIGZvciBkaWZmZXJlbnQgYXNrX3F1ZXN0aW9ucyBpbnRlcnZhbHNcbiMgRC4gRXJyb3IuIn0= 4.6.4 A complex example eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuXG5tb29keSRjb25kaXRpb25hbCA8LTBcbm1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb248MC41MCwgXSRjb25kaXRpb25hbCA8LSBtb29keVttb29keSRwYXJ0aWNpcGF0aW9uPDAuNTAsIF0kc2NvcmUgLTEwKm1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb248MC41MCwgXSRwYXJ0aWNpcGF0aW9uXG5tb29keVttb29keSRwYXJ0aWNpcGF0aW9uPj0wLjUwLCBdJGNvbmRpdGlvbmFsIDwtIG1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb24+PTAuNTAsIF0kc2NvcmUgKzEwKm1vb2R5W21vb2R5JHBhcnRpY2lwYXRpb24+PTAuNTAsIF0kcGFydGljaXBhdGlvblxuXG5zdW1tYXJ5KG1vb2R5JGNvbmRpdGlvbmFsKVxuXG5ib3hwbG90KG1vb2R5JGNvbmRpdGlvbmFsLGNvbCA9IGMoXCJyZWRcIiksbWFpbj1cIkNvbXBsZXggRXhhbXBsZVwiKSJ9 4.7 What would R say? In this section we will look at few examples based on the question What do you think would R say? All the questions are based on what we have studied in the sections above. INSTRUCTIONS: Do not run the following examples directly, first ask yourself and note down, what do you think would R say? Only then run them. This is the only way to learn simple commands - and have them memorized so you can write code without having to check every single command. 4.7.1 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ3ZWF0aGVyID1kYXRhLmZyYW1lKERheT1jKCd3ZWVrZGF5JywgJ3dlZWtlbmQnKSwgQ29uZGl0aW9ucyA9Yygnc3VubnknLCdyYWlueScsJ2Nsb3VkeScsICdzbm93JywgJ3N0b3JtJywnaWNlJykpXG5kaW0od2VhdGhlcilcbiN3aGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEpIDYgNlxuIyBCKSAyIDZcbiMgQykgNiAyXG4jIEQpIEVycm9yIn0= 4.7.2 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ3ZWF0aGVyID1kYXRhLmZyYW1lKERheT1jKCd3ZWVrZGF5JywgJ3dlZWtlbmQnKSwgQ29uZGl0aW9ucyA9Yygnc3VubnknLCdyYWlueScsJ2Nsb3VkeScsICdzbm93JywgJ3N0b3JtJywnaWNlJykpXG53ZWF0aGVyJHRlbXBlcmF0dXJlID1jKDgwLCA3MCwgNjUsIDQwLCAzMCwyNSlcbndlYXRoZXJbd2VhdGhlciR0ZW1wZXJhdHVyZSA+IDQwLF1cbmRpbSh3ZWF0aGVyKVxuI3doYXQgd291bGQgUiBzYXk/XG5cbiMgQSkgNiAzXG4jIEIpIHN1YnNldCBvZiB0aGUgZGF0YWZyYW1lIHdpdGggdGVtcGVyYXR1cmUgPiA0MC5cbiMgQykgQm90aCBBIGFuZCBCXG4jIEQpIEVycm9yIn0= 4.7.3 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJTQ09SRT1jKDMwLDE1LDY2KTtcbkdSQURFPWMoJ0MnLCAnRicsICdBJylcbk9OX1NNQVJUUEhPTkU9YygnYWx3YXlzJywgJ25ldmVyJywgJ3NvbWV0aW1lcycpXG5GSU5BTEVYQU09YygxMiw1LDIwKVxuTT1kYXRhLmZyYW1lKFNDT1JFLCBHUkFERSwgT05fU01BUlRQSE9ORSwgRklOQUxFWEFNKVxuc3Vic2V0KE0sIEdSQURFPT0nRicpXG4jd2hhdCB3b3VsZCBSIHNheT9cblxuIyBBKSBTdWJzZXQgb2YgZGF0YWZyYW1lIGJhc2VkIG9uIEdyYWRlIGVxdWFsIHRvIEZcbiMgQikgU3Vic2V0IG9mIHRoZSBkYXRhZnJhbWUgYmFzZWQgb24gR3JhZGUgbm90IGVxdWFsIHRvIEZcbiMgQykgdGhlIGNvbXBsZXRlIGRhdGFmcmFtZVxuIyBEKSBFcnJvciJ9 4.7.4 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJTQ09SRT1jKDMwLDE1LDY2KTtcbkdSQURFPWMoJ0MnLCAnRicsICdBJylcbk9OX1NNQVJUUEhPTkU9YygnYWx3YXlzJywgJ25ldmVyJywgJ3NvbWV0aW1lcycpXG5GSU5BTEVYQU09YygxMiw1LDIwKVxuTT1kYXRhLmZyYW1lKFNDT1JFLCBHUkFERSwgT05fU01BUlRQSE9ORSwgRklOQUxFWEFNKVxuTVtGSU5BTEVYQU0gPiA1LF1cbiN3aGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEpIFN1YnNldCBvZiBkYXRhZnJhbWUgd2l0aCBmaW5hbGV4YW0gdmFsdWVzIGdyZWF0ZXIgdGhhbiBlcXVhbCB0byA2XG4jIEIpIFN1YnNldCBvZiBkYXRhZnJhbWUgd2l0aCBmaW5hbGV4YW0gdmFsdWVzIGdyZWF0ZXIgdGhhbiBlcXVhbCB0byA1XG4jIEMpIFN1YnNldCBvZiBkYXRhZnJhbWUgd2l0aCBmaW5hbGV4YW0gdmFsdWVzIGxlc3MgdGhhbiA1LiJ9 4.7.5 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJTQ09SRT1jKDMwLDE1LDY2KVxuR1JBREU9YygnQycsICdGJywgJ0EnKVxuT05fU01BUlRQSE9ORT1jKCdhbHdheXMnLCAnbmV2ZXInLCAnc29tZXRpbWVzJylcbkZJTkFMRVhBTT1jKDEyLDUsMjApXG5NPWRhdGEuZnJhbWUoU0NPUkUsIEdSQURFLCBPTl9TTUFSVFBIT05FLCBGSU5BTEVYQU0pXG5NJFFVRVNUSU9OUz0nbm9uZSdcbk1bLDVdXG4jd2hhdCB3b3VsZCBSIHNheT9cblxuIyBBKSBPdXRwdXQgdGhlIGNvbnRlbnQgb2YgYWxsIHRoZSBjb2x1bW5zXG4jIEIpIE91dHB1dCB3b3JkIFwibm9uZVwiIGZvciAzIHRpbWVzIFxuIyBDKSBPdXRwdXQgd29yZCBcIm5vbmVcIiBmb3IgNSB0aW1lc1xuIyBEKSBFcnJvciJ9 4.7.6 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJTQ09SRT1jKDMwLDE1LDY2KVxuR1JBREU9YygnQycsICdGJywgJ0EnKVxuT05fU01BUlRQSE9ORT1jKCdhbHdheXMnLCAnbmV2ZXInLCAnc29tZXRpbWVzJylcbkZJTkFMRVhBTT1jKDEyLDUsMjApXG5NPWRhdGEuZnJhbWUoU0NPUkUsIEdSQURFLCBPTl9TTUFSVFBIT05FLCBGSU5BTEVYQU0pXG50YWJsZShNJFNDT1JFPjE1LCBNJEdSQURFKVxuI3doYXQgd291bGQgUiBzYXk/XG5cbiMgQSkgT3V0cHV0IHRoZSB0YWJsZSBvZiBjb3VudCBvZiBTY29yZSBncmVhdGVyIHRoYW4gMTUgdnMgR3JhZGVcbiMgQikgT3V0cHV0IHRoZSB0YWJsZSBvZiBjb3VudCBvZiBzY29yZSBncmVhdGVyIHRoYW4gMTUgb25seVxuIyBDKSBPdXRwdXQgdGhlIHRhYmxlIG9mIGNvdW50IG9mIGdyYWRlcyBvbmx5XG4jIEQpIE91dHB1dCB0aGUgdGFibGUgb2YgZ3JhZGUgZGlzdHJpYnV0aW9uIHZzIGFsbCBzY29yZS4ifQ== 4.7.7 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ1PC1jKDE6MTApXG53IDwtYygxLC0xLDMpXG51W3c+MF1cbiN3aGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEpICAxICAzICA0ICA2ICA3ICA5IDEwIFxuIyBCKSAgMSAgMiAgMyAgNCAgNSAgNiAgNyAgOCAgOSAgMTBcbiMgQykgIDEgIDMgIDEgIDMgIDEgIDMgIDEgIDMgIDEgIDMgXG4jIEQpIEVycm9yIn0= 4.7.8 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2IDwtIGMoLTIsMCwyLC01KVxudlt2PjBdXG4jd2hhdCB3b3VsZCBSIHNheT9cblxuIyBBKSAyXG4jIEIpIDAgMlxuIyBDKSBGQUxTRSBGQUxTRSAgVFJVRSBGQUxTRVxuIyBEKSBFcnJvciJ9 4.7.9 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJjKFwiYVwiLDEsVClcbiN3aGF0IHdvdWxkIFIgc2F5P1xuXG4jIEEpIE5hTiAgMSAgTmFOXG4jIEIpIFwiYVwiICAxICBUXG4jIEMpIFwiYVwiICBcIjFcIiAgXCJUUlVFXCJcbiMgRCkgXCJhXCIgIFwiMVwiICBcIlRcIiJ9 4.7.10 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ4PC0xOjRcbnk8LTI6OVxueCt5XG4jd2hhdCB3b3VsZCBSIHNheT9cblxuIyBBKSAzICA1ICA3ICA5ICA3ICA5IDExIDEzXG4jIEIpIDEgIDIgIDMgIDQgIDIgIDMgIDQgIDUgIDYgIDcgIDggIDlcbiMgQykgMyAgNSAgNyAgOVxuIyBEKSBFcnJvciJ9 4.7.11 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2MTwtIGMoMSwyLDMsNClcbnYyPC0gYygxLDIsMyw0KVxudjM8LSBjKDEsMiwzLDQpXG5cbmRmPC1kYXRhLmZyYW1lKHYxLHYyLHYzKVxuIzFcbmRmW2RmPjJdICBcblxuIyBBLiB2YWx1ZXMgb2YgdjEgdmFyaWFibGUgd2hpY2ggYXJlIGxhcmdlciB0aGFuIDIgXG5cbiMgQi4gdmFsdWVzIG9mIHYxLCB2MiBhbmQgdjMgd2hpY2ggYXJlIGxhcmdlciB0aGFuIDIuIFxuXG4jIEMuIGVycm9yICJ9 4.7.12 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2MTwtIGMoMSwyLDMsNClcbnYyPC0gYygxLDIsMyw0KVxudjM8LSBjKDEsMiwzLDQpXG5cbmRmPC1kYXRhLmZyYW1lKHYxLHYyLHYzKVxuXG5kZiR2MSA+IDIgXG5cbiMgQS4gZXJyb3IgXG5cbiMgQi4gdmFsdWVzIG9mIHYxIHZhcmlhYmxlIHdoaWNoIGFyZSBsYXJnZXIgdGhhbiAyLlxuXG4jIEMuIFRSVUUgd2hlcmUgdGhlIHZhbHVlIGlzIGdyZWF0ZXIgdGhhbiAyIGFuZCBGYWxzZSB3aGVyZSB0aGUgdmFsdWUgaXMgbGVzcyB0aGFuIDIuICJ9 4.7.13 Question eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ2MTwtIGMoMSwyLDMsNClcbnYyPC0gYygxLDIsMyw0KVxudjM8LSBjKDEsMiwzLDQpXG5cbmRmPC1kYXRhLmZyYW1lKHYxLHYyLHYzKVxuXG52MT4yXG5cbiMgQS4gVFJVRSB3aGVyZSB0aGUgdmFsdWUgaXMgZ3JlYXRlciB0aGFuIDIgYW5kIEZhbHNlIHdoZXJlIHRoZSB2YWx1ZSBpcyBsZXNzIHRoYW4gMi5cblxuIyBCLiB2YWx1ZXMgb2YgdjEgdmFyaWFibGUgd2hpY2ggYXJlIGxhcmdlciB0aGFuIDJcblxuIyBDLiBlcnJvciJ9 "],["datatransformation.html", "Chapter 5 Data Frames &amp; Transformation. 5.1 Create Column 5.2 Factor Function: factor() 5.3 Coercing Values in data frames 5.4 Merging Two Relational Data Frames. 5.5 Slicing and Dicing. 5.6 Group By 5.7 Handling Date and Time in dataframes.", " Chapter 5 Data Frames &amp; Transformation. Now we have to introduce the core data structure of R  the data frame and show we can expand it with extra attributes. Defining new attributes can very often be critical in data exploration and help to find patterns and relationships which otherwise would not be visible. For example, may be participation matters but only to Pass/Fail grades? In other words students who Pass (A or B or C) always have participation above a certain threshold? Perhaps students who always text never pass the class? And students who always ask questions never fail? Such rules can only be discovered if we define a new Pass/Fail attribute, additional to grade attribute. Similarly intervals of participation or score may discover important relationships which would not emerge with just numerical values of such attributes. May be High scores correlate with High participation? To establish it one would have first to define categorical attributes with named intervals of their numerical counterparts. 5.1 Create Column Lets put a column I have created using score. Suppose I am given a new column \" pf \" with same number of rows as that of the dataframe with the categories (P , F). eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiMgcGYgY29sdW1uIGhhcyAyIGNhdGVnb3J5IGFuZCBkaXZpZGVzIG9uIHRoZSBiYXNpcyBvZiBzY29yZS5cbnBmIDwtIGN1dChtb29keSRzY29yZSxicmVha3M9YygwLDUwLDEwMCksbGFiZWxzPWMoXCJGXCIsXCJQXCIpKVxuIyBsZW5ndGgocGYpICMgTnVtYmVyIG9mIHJvd3MgaW4gbmV3IGNvbHVtbi5cbiMgbnJvdyhtb29keSkgIyBOdW1iZXIgb2YgUm93cyBpbiBkYXRhZnJhbWVcblxuIyBUbyBhZGQgdGhpcyBuZXcgY29sdW1uIHBmIGluIGRhdGFmcmFtZSBtb29keS5cbm5hbWVzKG1vb2R5KSAjIEluaXRpYWxseSBkYXRhZnJhbWUgaGFzIDUgY29sdW1uc1xubW9vZHkkcGFzc2ZhaWwgPC0gcGYgI1B1dCBzeW50YXggZGF0YUZyYW1lTmFtZSRjb2x1bW5IZWFkZXJOYW1lIDwtIG5ld0NvbHVtblxubmFtZXMobW9vZHkpICMgTm93IGRhdGFmcmFtZSBoYXMgNiBjb2x1bW5zIn0= eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIG1vb2R5PC1yZWFkLmNzdihcIi4uL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3N0YXRpYyBMb2FkXG5tb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkXG5cbiNXaGF0IGhhcHBlbnMgd2hlbiB5b3UgaGF2ZSBjb2x1bW4gc2l6ZSBtaXNtYXRjaC5cbmJhZGNvbCA8LSBjKDE6MTApXG5sZW5ndGgoYmFkY29sKVxuXG5tb29keSRiYWRjb2wgPC0gYmFkY29sICNUaHJvd3MgQ29tcGF0aWJpbGl0eSBlcnJvci4gIn0= 5.2 Factor Function: factor() Factors are the data objects which are used to categorize the data and store it as levels. They can store both strings and numbers. They are useful in the columns which have a limited number of unique values. Like Male,Female\" and True, False etc. Factor data objects are useful in data analysis for statistical modeling. The factor function is used to encode a vector as a factor. Lets look at first example, checking if a data object is of factor type using the function is.factor(x) eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIENyZWF0ZSBhIHZlY3RvciBhcyBpbnB1dC5cbmdlbmRlciA8LSBjKFwibWFsZVwiLFwibWFsZVwiLFwiZmVtYWxlXCIsXCJmZW1hbGVcIixcIm1hbGVcIixcImZlbWFsZVwiLFwibWFsZVwiKVxuXG5nZW5kZXJcblxuI0NoZWNrIGlmIGRhdGEgb2JqZWN0IGlzIGZhY3Rvci5cbmlzLmZhY3RvcihnZW5kZXIpIn0= Now lets convert the above vector to a factor data object. To do this we will use the function factor(x). eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIENyZWF0ZSBhIHZlY3RvciBhcyBpbnB1dC5cbmdlbmRlciA8LSBjKFwibWFsZVwiLFwibWFsZVwiLFwiZmVtYWxlXCIsXCJmZW1hbGVcIixcIm1hbGVcIixcImZlbWFsZVwiLFwibWFsZVwiKVxuXG4jIEFwcGx5IHRoZSBmYWN0b3IgZnVuY3Rpb24uXG5mYWN0b3JfZ2VuZGVyIDwtIGZhY3RvcihnZW5kZXIpXG5cbmZhY3Rvcl9nZW5kZXJcbmlzLmZhY3RvcihmYWN0b3JfZ2VuZGVyKSJ9 Notice that for the factor data objects, the attribute Levels is also created. This is an extremely important feature of the factor data object. Lets look at how the factor data object looks when included in a dataframe. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIENyZWF0ZSB0aGUgdmVjdG9ycyBmb3IgZGF0YSBmcmFtZS5cbmhlaWdodCA8LSBjKDEzMiwxNTEsMTYyLDEzOSwxNjYsMTQ3LDEyMilcbndlaWdodCA8LSBjKDQ4LDQ5LDY2LDUzLDY3LDUyLDQwKVxuZ2VuZGVyX25vdF9mYWN0b3IgPC0gYyhcIm1hbGVcIixcIm1hbGVcIixcImZlbWFsZVwiLFwiZmVtYWxlXCIsXCJtYWxlXCIsXCJmZW1hbGVcIixcIm1hbGVcIilcblxuIyBDT252ZXJ0IHRoZSBnZW5kZXJfbm90X2ZhY3RvciB2ZWN0b3IgdG8gYSBmYWN0b3IgZGF0YSBvYmplY3QuXG5nZW5kZXIgPC0gZmFjdG9yKGdlbmRlcl9ub3RfZmFjdG9yKVxuXG4jIENyZWF0ZSB0aGUgZGF0YSBmcmFtZS5cbmlucHV0X2RhdGEgPC0gZGF0YS5mcmFtZShoZWlnaHQsd2VpZ2h0LGdlbmRlcilcbnByaW50KGlucHV0X2RhdGEpXG5cbiMgVGVzdCBpZiB0aGUgZ2VuZGVyIGNvbHVtbiBpcyBhIGZhY3Rvci5cbnByaW50KGlzLmZhY3RvcihpbnB1dF9kYXRhJGdlbmRlcikpXG5cbiMgUHJpbnQgdGhlIGdlbmRlciBjb2x1bW4gc28gc2VlIHRoZSBsZXZlbHMuXG5wcmludChpbnB1dF9kYXRhJGdlbmRlcikifQ== Note: Sometimes depending on your version of R and packages, you might find that while inserting categorical vector into the data frame using the data.frame() function, without converting the categorical vector to factor, it automatically gets converted into a factor column. But to avoid confusion, it is a better technique to convert the categorical vector into factor using factor() function and then insert it in the data frame. Lets look at an example where the use of factor data object turns out to be useful. We have a categorical vector that we want to coerce as numeric for use in some model/application. Lets look at what happens when we just have a categorical vector, and we try to coerce it to numeric vector. We see that the outcome of the as.numeric() function on a normal categorical vector is coercion to NA of all elements. But when we convert the same categorical vector to factor, then after coercion to numeric type, we get a numeric vector of elements corresponding the the index of the labels of the factor data object. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJnZW5kZXJfbm90X2ZhY3RvciA8LSBjKFwibWFsZVwiLFwibWFsZVwiLFwiZmVtYWxlXCIsXCJmZW1hbGVcIixcIm1hbGVcIixcImZlbWFsZVwiLFwibWFsZVwiKVxuZ2VuZGVyX25vdF9mYWN0b3JcblxuIyBDb2VyY2UgaW50byBudW1lcmljIHZlY3RvciB3aXRob3V0IGNvbnZlcnRpbmcgdG8gZmFjdG9yXG5hcy5udW1lcmljKGdlbmRlcl9ub3RfZmFjdG9yKVxuXG5cbiMgQ09udmVydCB0aGUgZ2VuZGVyX25vdF9mYWN0b3IgdmVjdG9yIHRvIGEgZmFjdG9yIGRhdGEgb2JqZWN0LlxuZ2VuZGVyIDwtIGZhY3RvcihnZW5kZXJfbm90X2ZhY3RvcilcbmdlbmRlclxuXG4jIENvZXJjZSBpbnRvIG51bWVyaWMgdmVjdG9yIGFmdGVyIGNvbnZlcnRpbmcgdG8gZmFjdG9yIGRhdGEgb2JqZWN0LlxuYXMubnVtZXJpYyhnZW5kZXIpIn0= Lets look at another example where factor is useful. We want to see the distribution of price of each quality for the wine dataset. Upon plotting, it gives us a scatter plot, which makes it hard for us to see the distribution. Thus we convert the quality vector which is numeric initially, to factor and the plot it again. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJ3aW5lIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0LzU0MVdJTkUuY3N2XCIpXG5wbG90KHdpbmUkUVVBTElUWSx3aW5lJFBSSUNFKVxuI3dlIHdhbnQgdG8gc2VlIHRoZSBkaXN0cmlidXRpb24gb2YgcHJpY2Ugb2YgZWFjaCBxdWFsaXR5LCBidXQgaXQgZ2l2ZXMgdXMgYSBzY2F0dGVyIHBsb3QsIHdoaWNoIG1ha2VzIGl0IGhhcmQgZm9yIHVzIHRvIHNlZSB0aGUgZGlzdHJpYnV0aW9uLlxuaXMuZmFjdG9yKHdpbmUkUVVBTElUWSlcbiN0aGUgcmVzdWx0IGlzIGZhbHNlLCB3aGljaCBtZWFucyBxdWFsaXR5IGlzIGEgbnVtZXJpYyB2YWx1ZSByYXRoZXIgdGhhbiBhIGZhY3RvclxuXG5mYWN0b3JfcXVhbGl0eSA8LSBmYWN0b3Iod2luZSRRVUFMSVRZKVxuI2NvbnZlcnQgcXVhbGl0eSB2YWx1ZXMgaW50byBmYWN0b3JzXG5wbG90KGZhY3Rvcl9xdWFsaXR5LHdpbmUkUFJJQ0UpXG4jbm93IHdlIGNhbiBnZW5lcmF0ZSB0aGUgYm94IHBsb3QgYW5kIHNlZSB0aGUgZGlzdHJpYnV0aW9uIGNsZWFybHkuIn0= 5.3 Coercing Values in data frames Before coercing data into data frames, lets look at small examples. Lets look at a coerced vector. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjTGV0cyBsb29rIGF0IGEgY29lcmNlZCB2ZWN0b3IuXG5cbiN2ZWN0b3IgY29udGFpbmluZyA0IGVsZW1lbnRzXG5teVZlY3Q8LWMoXCJSb2JlcnRcIiwgXCJFdGhhblwiLCA2LCA0KVxubXlWZWN0XG5cbiNZb3Ugd2lsbCBub3RpY2UgdGhhdCB0aGUgbGFzdCB0d28gZWxlbWVudHMgLCB3aGljaCBhcmUgYW4gaW50ZWdlcnMsIGFyZSBjb2VyY2VkIGludG8gYSBjaGFyYWN0ZXIgdHlwZS5cblxuI2NsYXNzKCkgaXMgdXNlZCB0byBjaGVjayB0aGUgdHlwZSBvZiBhbiBvYmplY3RcbmNsYXNzKG15VmVjdCkifQ== We see that when a vector has elements of mixed data types, they gets coerced into a type with precedence over other types. For example in the above case there were character elements and numeric elements types in the vector. But character type has precedence over numeric type and hence the whole vector is coerced into character type. We can check the types of vectors using a specific type of is function: is.character(), is.double(), is.integer(), is.logical(),etc. There are many other types under the is function, for checking if the data object given is a dataframe, factor, etc. Lets look at the examples. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjdmVjdG9yIGNvbnRhaW5pbmcgNCBlbGVtZW50c1xubXlWZWN0PC1jKFwiUm9iZXJ0XCIsIFwiRXRoYW5cIiwgNiwgNClcbm15VmVjdFxuXG4jIENoZWNrIGlmIHZlY3RvciBpcyBvZiBDaGFyYWN0ZXIgdHlwZS5cbmlzLmNoYXJhY3RlcihteVZlY3QpXG5cbiMgQ2hlY2sgaWYgdmVjdG9yIGlzIG9mIG51bWVyaWMgdHlwZS5cbmlzLm51bWVyaWMobXlWZWN0KVxuXG4jIFVzZSBUUlVFIGFuZCBGQUxTRSAob3IgVCBhbmQgRikgdG8gY3JlYXRlIGxvZ2ljYWwgdmVjdG9yc1xubG9nX3ZlYyA8LSBjKFRSVUUsIEZBTFNFLCBULCBGKVxuXG4jIENoZWNrIGlmIHZlY3RvciBpcyBvZiBsb2dpY2FsIHR5cGUuXG5pcy5sb2dpY2FsKGxvZ192ZWMpIn0= We saw how to check the type of the data. But if you want to convert a column into your choice of data type, you can use the specific type of as function: as.character(), as.double(), as.integer(), as.logical(),etc. Again as we saw above about the is function types, there are also many other types of the as function. Lets look at the example of coercing a vector into character type. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjdmVjdG9yIGNvbnRhaW5pbmcgNCBlbGVtZW50c1xubXlWZWN0PC1jKDIsIDMsIDYsIDQsIFRSVUUsIEZBTFNFKVxubXlWZWN0XG5cbiMgRmlyc3QgbGV0cyBsb29rIGF0IHRoZSBjbGFzcyBvZiB0aGUgdmVjdG9yXG5jbGFzcyhteVZlY3QpXG5cbiMgQ29lcmNlIHRoZSB2ZWN0b3IgdG8gQ2hhcmFjdGVyIHR5cGUuIFxuYXMuY2hhcmFjdGVyKG15VmVjdCkgXG5cbiMgWW91IGNhbiBzZWUgdGhhdCB0aGUgZWxlbWVudHMgb2YgdGhlIG51bWVyaWMgdmVjdG9yIGFyZSBjb2VyY2VkIGludG8gY2hhcmFjdGVyIHR5cGUuIn0= Lets look at an example of coercing a mixed type vector into numeric type. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJteXZlYzwtYyhcIlJvYmVydFwiLCBcIjIyXCIsIDQ1KVxubXl2ZWNcblxuIyBGaXJzdCBsZXRzIGxvb2sgYXQgdGhlIGNsYXNzIG9mIHRoZSB2ZWN0b3JcbmNsYXNzKG15dmVjKVxuXG4jIENvZXJjZSB0aGUgY2hhcmFjdGVyIHZlY3RvciB0byBudW1lcmljIHR5cGUuIFxuYXMubnVtZXJpYyhteXZlYykgXG5cbiMgWW91IGNhbiBzZWUgdGhhdCB0aGUgZWxlbWVudHMgb2YgdGhlIG1peGVkIHZlY3RvciBhcmUgY29lcmNlZCBpbnRvIG51bWVyaWMgdHlwZS5cblxuXG5cbm15dmVjMiA8LSBjKFRSVUUsIEZBTFNFLCBGLCBULCBUKVxubXl2ZWMyXG5cbiMgRmlyc3QgbGV0cyBsb29rIGF0IHRoZSBjbGFzcyBvZiB0aGUgdmVjdG9yXG5jbGFzcyhteXZlYzIpXG5cbiMgQ29lcmNlIHRoZSBsb2dpY2FsIHZlY3RvciB0byBudW1lcmljIHR5cGUuIFxuYXMubnVtZXJpYyhteXZlYzIpIFxuXG4jIFlvdSBjYW4gc2VlIHRoYXQgdGhlIGVsZW1lbnRzIG9mIHRoZSBtaXhlZCB2ZWN0b3IgYXJlIGNvZXJjZWQgaW50byBudW1lcmljIHR5cGUuIn0= We can see in the above example, while converting the character type vector to numeric if we encounter, numbers in character type, they get converted to numeric type. But the characters in character type, are not not converted, and instead we get a warning saying NAs introduced by coercion. Also, while converting a logical vector to numeric vector, we see that TRUE or T is coerced as 1 and FALSE or F is coerced as 0. Now lets look at how to coerce data column and rewrite it into the dataframe. Suppose in the Moody dataset, you want to change the categorical vector of letter grade to numeric grades between 1 to 5, where A=1, B=2, , F=5. First, you will convert the grade column vector to factor using the factor() function. Then, convert the grade column with the command as.numeric() to numeric column. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBDb252ZXJ0IHRoZSBjYXRlZ29yaWNhbCBjb2x1bW4gZ3JhZGUgdG8gZmFjdG9yIGRhdGEgY29sdW1uLlxubW9vZHkkZ3JhZGU8LWZhY3Rvcihtb29keSRncmFkZSlcbmhlYWQobW9vZHkkZ3JhZGUpXG5cbiMgTm93IGNvbnZlcnQgdGhlIGxldmVscyB0byBudW1lcmljIHVzaW5nIHRoZSBhcy5udW1lcmljIGZ1bmN0aW9uXG5tb29keSRncmFkZSA8LSBmYWN0b3IoYXMubnVtZXJpYyhtb29keSRncmFkZSkpXG5oZWFkKG1vb2R5JGdyYWRlKSJ9 We can see that the outcome of the above code, gives us a moody dataframe with grade column as a numeric column converted from the previous categorical column. We can also see that the we used the as.numeric() function inside the factor function while converting from categorical to numeric, to maintain the levels information of the grade column. Now, suppose you also want to change the labels of the grade column. Lets change the grades from capital letters to small letters, i.e. A -&gt; a, B -&gt; b, and so on. To do this, we can provide our user defined labels vector to the labels attribute of the factor() function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBDb252ZXJ0IHRoZSBjYXRlZ29yaWNhbCBjb2x1bW4gZ3JhZGUgdG8gZmFjdG9yIGRhdGEgY29sdW1uIHdpdGggdXNlciBkZWZpbmVkIGxhYmVscy5cbm1vb2R5JGdyYWRlIDwtIGZhY3Rvcihtb29keSRncmFkZSxsYWJlbHMgPSBjKFwiYVwiLFwiYlwiLFwiY1wiLFwiZFwiLFwiZlwiKSlcbmhlYWQobW9vZHkkZ3JhZGUpIn0= We can see that the capital letter are now transformed to small letters. 5.4 Merging Two Relational Data Frames. Often, we have data from multiple sources/multiple databases, files etc. To perform analysis, we need to merge these dataframes together with one or more common key variables. In R the merge() function allows merging two data frames by common columns or row names. This function allows you to perform different SQL joins, like left join, inner join, right join or full join, among others. We will look at merging datasets in R with this function, along with examples. Consider the following 2 datasets. First is a smaller just 4 record data subset of the Moody dataset. Table 5.1: Small subset of Moody Dataset STUDENTID SCORE GRADE ON_SMARTPHONE ASKS_QUESTIONS FINALEXAM 65446 23.67 D never always 12.874804 79686 8.41 F never never 5.044093 56400 69.76 C never always 23.585730 16792 95.51 A never always 23.476748 Second is another dataset of students with respective GPA and Majors. Table 5.2: Small dataset of students information STUDENTID GPA Major 65446 1.559626 computer science 79686 3.813033 economics 56400 2.840912 political science 10001 2.664000 economics NOTE: We can see from the above snippets of the above the top 3 records in both dataset have same STUDENTID, but the 4th records in both datasets are of different students. The most important element while discussing the examples below, will focus on what happens to the 4th records of both datasets when using the various merge options and attributes. 5.4.1 Inner Join This is the most usual type of join of datasets that you can perform. It consists of merging two dataframes in one that contains common elements of both. In order to merge the two datasets, you just have to pass them to the merge() function without the need of changing other arguments. Inner join merge is the default merge of the merge() function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cbiMgVXNlIHRoZSBtZXJnZSBmdW5jdGlvbiwgd2l0aG91dCBhbnkgYXR0cmlidXRlcy5cbm1lcmdlKG1vb2R5X2RmLHN0dWRlbnRfZGYpIn0= We can see that there are only 3 record in the output. The reason being that, the studentid of the fourth record in both the dataset did not match. And thus the merge function did not know which datasets record to be kept and which not. Also the reason the merge function tried to match and merge the two datasets, is by using the first columns from both the datasets, which in both case was the STUDENTID column. We can also do the same process, and get he same outcome, by defining the index column by yourself. Lets look at this in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cbiMgVXNlIHRoZSBtZXJnZSBmdW5jdGlvbiwgd2l0aCB0aGUgXCIgYnkgXCIgIGF0dHJpYnV0ZS5cbm1lcmdlKG1vb2R5X2RmLHN0dWRlbnRfZGYsYnkgPSBcIlNUVURFTlRJRFwiKSJ9 As we can see, the output remains the same. But we understand that we can define any other common column as the index column based on which the merging can occur. IMPORTANT NOTE: There are also arguments like by.x and by.y which correspond to indexing based on one of the column from the left(first) or right(second) datasets respectively. This could come extremely handy, when the two datasets you want to merge, have different column name for the index column. For example, suppose in the two dataset that we have considered above, the first dataset had students records indexed by the studentid column where the indexing column name is studentid, but in the second dataset the indexing column even though with same student ids as entries but with the column name of stu-id. Now while merging, you can face error since the merge() function will have trouble finding the two index columns to match since they are named differently in the two datasets. Here you can provide the argument by.x = \"studentid\" , by.y = \"stu-id\" in the function while merging. 5.4.1.1 Another example Suppose you have the happiness index dataset, Table 5.3: Happiness Index Dataset for all countries IDN AGE COUNTRY GENDER IMMIGRANT INCOME HAPPINESS 88364 29 Kyrgyzstan Male 0 103305 8.35 37692 41 Afghanistan Male 0 51682 4.44 57856 20 Azerbaijan Female 0 72381 6.24 49453 62 South Korea Female 0 65658 5.66 93485 63 Jordan Female 1 109581 3.17 97976 36 Congo-Kinshasa Female 0 112432 9.43 where you have the survey data of people of various countries with records of information about AGE, COUNTRY, GENDER, IMMIGRANT, INCOME, and HAPPINESS. You can do analysis on the above dataset per country, per age group,etc. But if you want to do analysis based on per continent, then you will have to create lists of all the countries in each continent, and then subset using the appropriate subset method/s from section below 5.5. Alternate method will be acquiring another dataset, with information of each country and its respective continent, and do merge, which we can then use to subset easily. Lets look at an example of this process below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCB0aGUgSGFwcGluZXNzIGluZGV4IGRhdGFzZXQuXG5oYXBweTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvSEFQUElORVNTMjAxNy5jc3ZcIilcbmhlYWQoaGFwcHkpXG5cbiMgTm93IGxldHMgbG9hZCB0aGUgc2ltcGxlIGRhdGFzZXQgb2YgY291bnRyeSBhbmQgY29udGluZW50cy5cbmNvbnRpbmVudHM8LXJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L2NvdW50cnktY29udGluZW50cy5jc3ZcIilcbmhlYWQoY29udGluZW50cylcblxuIyBOb3cgd2UgY2FuIHVzZSB0aGUgbWVyZ2UgZnVuY3Rpb24gdG8gaW5jbHVkZSB0aGUgY29udGluZW50cyBvZiBlYWNoIGNvdW50cnkgaW4gdGhlIGhhcHBpbmVzcyBkYXRhc2V0IGFnYWluc3QgZWFjaCBvdGhlci5cbmhhcHB5LmM8LW1lcmdlKGhhcHB5LGNvbnRpbmVudHMpXG5oZWFkKGhhcHB5LmMpXG5cbmhhcHB5LmNbc2FtcGxlKG5yb3coaGFwcHkuYyksMTApLF0ifQ== We can see from the output of the above example, the new dataframe created in happy.c after applying merge() function on the Happiness index dataset and the country-continents dataset, the CONTINENT column is added from the country-continents dataset into the happyness index dataset. And each country in the happy.c dataframe has now the value of its respective continent in the the CONTINENT column. 5.4.2 Full Join Full Join is also known as the outer join or the full outer join. It merges all the columns of both datasets into one. For those records with non-intersecting index elements, Full join keeps both the records, and fills the missing values with NA , i.e. Not Available(NA) keyword. In order to create this type of full join of the two dataframes in R, we need to set the argument all to TRUE or T. Lets look at this in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cbiMgVXNlIHRoZSBtZXJnZSBmdW5jdGlvbiwgd2l0aCB0aGUgXCIgYWxsIFwiICBhdHRyaWJ1dGUuXG5tZXJnZShtb29keV9kZixzdHVkZW50X2RmLGFsbCA9IFRSVUUpIn0= We can see that the first record of the output with studentid = 10001 was present in the second dataset only, thus the values corresponding to the columns of the first dataset are set to NA. Similarly, the same occurs with the record with studentid = 16792, which was only present in the first dataset, and thus has NA in the place of columns of second dataset. 5.4.3 Left Join The left join in R involves matching all the rows in the first data frame with the corresponding records on the second dataframe. To create this left join, you just have to set the argument all.x to TRUE or T. Recall while doing the full join, we set the argument all to TRUE or T. Similarly, since we consider x as the first dataset or the left dataset, we will set the argument of all.x where the .x is the key to select the first dataset. We have seen in the snippets above, the student with studentid = 16792 is only present in the first dataset but not the second. So lets see the result of merging using the left join in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cbiMgVXNlIHRoZSBtZXJnZSBmdW5jdGlvbiwgd2l0aCB0aGUgXCIgYWxsIFwiICBhdHRyaWJ1dGUuXG5tZXJnZShtb29keV9kZixzdHVkZW50X2RmLGFsbC54ID0gVFJVRSkifQ== We can see that the record of student with studentid = 16792 has NA as the entry in the columns merged from the right dataset. Also, the record of student with studentid = 10001 is completely excluded, since it belongs to the second dataset. 5.4.4 Right Join The right join merge involves joining all the rows in the second data frame with the corresponding records on the first dataframe. The right join is opposite to that of left join. In consequence, here, you will need to set the argument all.y to TRUE or T, since we consider the right dataset or the second dataset as y. We have seen in the snippet above, that the student with studenid = 10001 is only present in the second dataset but not the first. So lets see the result of merging using the right join in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBEYXRhc2V0cy5cbm1vb2R5X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsTW9vZHkuY3N2XCIpXG5zdHVkZW50X2RmIDwtIHJlYWQuY3N2KFwiaHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2RlZXBsb2toYW5kZS9kYXRhMTAxZGVtb2Jvb2svbWFpbi9maWxlcy9kYXRhc2V0L1NtYWxsU3QuY3N2XCIpXG5cblxuIyBVc2UgdGhlIG1lcmdlIGZ1bmN0aW9uLCB3aXRoIHRoZSBcIiBhbGwgXCIgIGF0dHJpYnV0ZS5cbm1lcmdlKG1vb2R5X2RmLHN0dWRlbnRfZGYsYWxsLnkgPSBUUlVFKSJ9 We can see that the record of student with studentid = 10001 has NA as the entry in the columns merged from the left dataset. Also, the record of student with studentid = 16792 is completely excluded, since it belongs to the first dataset. 5.5 Slicing and Dicing. R was made especially for data analysis and graphics. SQL was made especially for databases. They are allies in this field of data science. The data structure in R that most closely matches a SQL table is a data frame. The terms rows and columns are used in both. There is an R package called sqldf that allows you to use SQL commands to extract data from an R data frame. We will not use this package in the examples but look at a way the operations in SQL translate to basic R commands that we have studied in previous chapter 4. In R we have seen how subsetting of rows and columns happen using the subset function in earlier chapters 4.4. Please review this section before proceeding ahead. 5.5.1 Subsetting on Columns ( DICING ) So lets start with dicing the dataframe. In other words, lets look at subsetting operations on columns. Columns in SQL are also called fields. In R it is commonly called variables. In SQL the subset of columns is determined by SELECT statement. We can do these type of SQL operation in R using the normal subsetting method, either using the subset() function or using the square brackets [ ]. NOTE: In most of the examples below, to avoid printing of the complete dataset after any operations, we have used the head() function to truncate the output to only top 6 rows. However you can always remove the function or change the limit or output records to your choice by passing additional attribute n = user_defined_limit to the head() function. Just to recap subsetting on columns, 5.5.1.1 Subset single column. Remember: You can either use the column names or the column location index, to dice the dataframe. Suppose we want to subset the moody dataset only the grade column. Lets look at this example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBMZXRzIHN1YnNldCB0aGUgZ3JhZGUgY29sdW1uIGZvcm0gdGhlIG1vb2R5IGRhdGFzZXQgYW5kIGxvb2sgYXQgaXRzIGZpcnN0IGZldyBlbGVtZW50cy4uXG5oZWFkKG1vb2R5WywnZ3JhZGUnLGRyb3A9Rl0pXG5cbiMgV2l0aG91dCBgZHJvcD1GYCBpbiB0aGUgYXR0cmlidXRlLCB5b3Ugd2lsbCBnZXQgb25seSB0aGUgdmFsdWVzIG9mIHRoZSBjb2x1bW4uXG5oZWFkKG1vb2R5WywnZ3JhZGUnXSkifQ== We can see that only one column is selected form the dataframe. The drop = F attribute is provided to keep the dataframe structure. You can also see the effect of not using the drop = F attribute in the above example. Note: In some cases, where you want to use the subsetted column with other function, e.g. mean(subsetted_column) you must not use the drop=F attribute, otherwise it will result in error. 5.5.1.2 Subset multiple column. Suppose you want to subset multiple columns by name, you can create a vector or the column names you want to subset and then include it wile subsetting. Lets look at the example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBoZXJlIHdlIGNyZWF0ZSBhIHZlY3RvciBvZiBjb2x1bW4gbmFtZSB0aGF0IHdlIHdhbnQgdG8gc3Vic2V0LlxuY29sdW1uTmFtZXM8LSBjKFwiZ3JhZGVcIixcInNjb3JlXCIpXG5cbiMgSW5jbHVkaW5nIHRoZSBhYm92ZSB2ZWN0b3Igd2lsZSBzdWJzZXR0aW5nLlxuaGVhZChtb29keVssY29sdW1uTmFtZXNdKSJ9 We can see that only the two column of grade and score are kept in the subset. Similarly, we can include the multiple column names and get subset. 5.5.1.3 Subset on all columns You can get all the columns in the subset, by keeping the space after the comma blank. This gives the complete set of columns. Suppose you did some slicing on the dataframe and want to keep all the columns in the output, you can just keep the space after the comma blank while subsetting. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBoZXJlIHdlIGNhbiBzZWUgdGhhdCB3ZSBzbGljZWQgdGhlIGRhdGFmcmFtZSB0byBvbmx5IGtlZXAgdGhlIHJlY29yZHMgb2Ygc3R1ZGVudHMgd2l0aCBncmFkZSBcIkFcIi4gXG5oZWFkKG1vb2R5W21vb2R5JGdyYWRlPT1cIkFcIiwgXSlcblxuXG4jIFdlICB3aWxsIGxvb2sgYXQgc2xpY2luZyBpbiB0aGUgc3Vic2VxdWVudCBzZWN0aW9ucy4ifQ== 5.5.2 Subsetting on Rows ( SLICING ) Now that we have seen dicing Or subsetting on columns, which is similar to the select statement of SQL, we will now look at slicing on the dataframe. Or in other words subsetting on rows. There are many statements of SQL that does subsetting on rows, i.e. SELECT, WHERE, AND, OR, IN, LIKE, LIMIT, and many more. We will look at few of them, by implementing them using the basic R functions. 5.5.2.1 Subsetting based on single condition. We will look at a subsetting condition based on value. For subsetting based on value, you can use the relational operators e.g. &gt; , &lt; , &gt;= , &lt;= , == , etc between the attribute name and the value. Lets look at this in the following example. Suppose you want to keep all the observations of where score of students are greater than 80. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBoZXJlIHdlIGNhbiBzZWUgdGhhdCB3ZSBzbGljZWQgdGhlIGRhdGFmcmFtZSB0byBvbmx5IGtlZXAgdGhlIHJlY29yZHMgb2Ygc3R1ZGVudHMgd2l0aCBzY29yZSBncmVhdGVyIHRoYW4gODAuIFxuaGVhZChtb29keVttb29keSRzY29yZT44MCwgXSkifQ== We can see from the above result, the subset has only records of students having score greater than 80. This example is similar to using where statement in SQL. 5.5.2.2 Subsetting based on multiple conditions. Similar to the above example, suppose you want to subset based on multiple conditions. To do this, we will use the logical operators e.g. AND (\" &amp; \") , OR (\" | \") , NOT (\" ! \") between the various conditions. Lets look at an example for this type of subsetting. Suppose you want to slice the records of the moody dataset, based on two conditions: - Students with grade equal to \" A \" - AND - Students with score greater than 90. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBoZXJlIHdlIGNhbiBzZWUgdGhhdCB3ZSBzbGljZWQgdGhlIGRhdGFmcmFtZSB0byBvbmx5IGtlZXAgdGhlIHJlY29yZHMgb2Ygc3R1ZGVudHMgd2l0aCBzY29yZSBncmVhdGVyIHRoYW4gOTAgQU5EIHdpdGggZ3JhZGUgZXF1YWwgdG8gXCJBXCIgLiBcbmhlYWQobW9vZHlbbW9vZHkkc2NvcmU+OTAgJiBtb29keSRncmFkZSA9PSBjKFwiQVwiKSwgXSkifQ== We can see that the records of students with score greater than 90 and grade equal to A are kept, rest all records are removed. This example is similar to using the AND, OR, NOT clause in SQL. 5.5.2.3 Subset based on multiple values. We will look at subsetting the dataframe based on one condition with multiple values. Suppose you want to subset the moody dataset, based on the students grade, but you want to keep students records with grade equal to both B and C Well you can use multiple conditions as seen above with an AND clause between the two conditions with different values on the same variable/columns, but there is a simple and useful way to do this with just one conditional statement. We will make use of a vector of all the values that we want to use, and then assign this vector to the condition statement. Lets look at this in the following example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBsZXRzIGNyZWF0ZSBhIHZlY3RvciBvZiB2YWx1ZXMgcmVxdWlyZWQgaW4gdGhlIGNvbmRpdGlvbmFsIHN0YXRlbWVudFxuY29uZFZhbHVlczwtIGMoXCJCXCIsXCJDXCIpXG5cbiMgaGVyZSB3ZSBjYW4gc2VlIHRoYXQgd2Ugc2xpY2VkIHRoZSBkYXRhZnJhbWUgdG8gb25seSBrZWVwIHRoZSByZWNvcmRzIG9mIHN0dWRlbnRzIHdpdGggZ3JhZGUgZXF1YWwgdG8gXCJCXCIgb3IgXCJDXCIgLiBcbmhlYWQobW9vZHlbbW9vZHkkZ3JhZGUgPT0gY29uZFZhbHVlcywgXSlcbnVuaXF1ZShtb29keVttb29keSRncmFkZSA9PSBjb25kVmFsdWVzLCBdJGdyYWRlKVxuXG4jIHdlIGNhbiBhbHNvIGRpcmVjdGx5IHdyaXRlIHRoZSB2ZWN0b3Igd2l0aG91dCBhc3NpZ25pbmcgYSB2YXJpYWJsZS5cbmhlYWQobW9vZHlbbW9vZHkkZ3JhZGUgPT0gYyhcIkJcIixcIkNcIiksXSlcbnVuaXF1ZShtb29keVttb29keSRncmFkZSA9PSBjKFwiQlwiLFwiQ1wiKSxdJGdyYWRlKSJ9 We can see that the output has only records of students with grades B or C. And both the methods, result in same output. This example is similar to the IN operator of the SQL 5.5.2.4 Subset based on a partial/complete text/character. We will look at subsetting the dataset based on a specific pattern of text/characters. This type of subsetting proves useful in text columns where each record has one or more than one sentence, and you want to search for a particular keyword or pattern. Most simple example would be of a survey dateset, where each record in the dataset consists of text paragraph, answering the questions asked in the survey, and you want to figure out the count of particular keywords in each response. Lets look at an example based on the Happiness dataset. We would like to find the subset of countries with the letters \" and \" in their name. eg. Iceland, Uganda, Poland, etc. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJoYXBweTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvSEFQUElORVNTMjAxNy5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuIyBTdWJzZXQgdXNpbmcgdGhlIGdyZXAgZnVuY3Rpb24gdG8gZmluZCB0aGUgcGF0dGVybiBcImFuZFwiIGluIHRoZSBuYW1lcyBvZiB0aGUgY291bnRyaWVzXG5oZWFkKGhhcHB5W2dyZXAoXCJhbmRcIixoYXBweSRDT1VOVFJZLGlnbm9yZS5jYXNlID0gVCksXSlcbnVuaXF1ZShoYXBweVtncmVwKFwiYW5kXCIsaGFwcHkkQ09VTlRSWSxpZ25vcmUuY2FzZSA9IFQpLF0kQ09VTlRSWSkifQ== We can see the output has subset of the happy dataset with records of only those countries with the pattern and in its name. To do this we have used the grep() function, which is a really important function for finding patterns in text and data. We dont need to study this grep() function in detail, but one can find very good resources explaining it online. This example is similar to the LIKE operator in SQL. 5.6 Group By Now that we have done Slicing and Dicing, we will like to apply some functions and gain measured information form the subsets. Although there is no straightforward, direct/ one step function to perform the function as that of the GROUP BY from SQL, but we can get the required functionality, by combining various functions step by step from the R.7 commands list 2.2 and the things we learned in this section 5 and the revision section 4. This section will involve use of the table(), tapply() function to apply the functions like mean, count, sum, etc on the subsets categorical or numerical columns. More importantly, we will look at a very useful example below, which will tie together all that we have learned until now. Suppose you want to get the statistics/numbers of average scores per grade and frequency of students per grade, and then use this table afterwards. So the SQL query will look something like SELECT grade, avg(score) as averagescore, count(*) as student_number FROM moody GROUP BY grade. To implement this above query functionality in R we would fisrt need to use the tapply function to find get the average score per grade and frequency per grade, and then combine it. Lets look at this process in the code below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIikgI3dlYiBsb2FkIGRhdGFzZXRcblxuXG4jIENyZWF0ZSBhIHRhYmxlIG9mIGZyZXF1ZW5jeSBvZiBzdHVkZW50cyBwZXIgZ3JhZGUuXG5ncmFkZS5jb3VudCA8LSB0YXBwbHkobW9vZHkkZ3JhZGUsbW9vZHkkZ3JhZGUsbGVuZ3RoKVxuXG4jIENyZWF0ZSBhIHRhYmxlIG9mIGF2ZXJhZ2Ugb2Ygc3R1ZGVudHMgc2NvcmUgcGVyIGdyYWRlLlxuZ3JhZGUubWVhbiA8LSB0YXBwbHkobW9vZHkkc2NvcmUsbW9vZHkkZ3JhZGUsbWVhbilcblxuIyBXZSBub3cgY29tYmluZSB0aGUgdHdvIHRhYmxlcyB0b2dldGhlciB1c2luZyBjYmluZCBhbmQgc3RvcmUgaXQgYXMgZGF0YS5mcmFtZSBmb3Igc2ltcGxlIHBvc3QtcHJvY2Vzc2luZy5cbm91dDwtYXMuZGF0YS5mcmFtZShjYmluZChncmFkZS5jb3VudCxncmFkZS5tZWFuKSlcbm91dCJ9 We can see the combined table of both the average scores and frequency per grade. Now suppose we want to go one step ahead and want to order the out table from the example above, based on decreasing value of frequency of students per grade. To do this, we introduce the order() function. Order() Function The order() function returns a permutation of the order of the elements of a vector. You can decide by passing the argument to order the elements in ascending or descending order. An important thing to note, is that for our use for the example we discussed above, we will use the order function as a subset parameter. Lets look at this in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIilcbmdyYWRlLmNvdW50IDwtIHRhcHBseShtb29keSRncmFkZSxtb29keSRncmFkZSxsZW5ndGgpXG5ncmFkZS5tZWFuIDwtIHRhcHBseShtb29keSRzY29yZSxtb29keSRncmFkZSxtZWFuKVxub3V0PC1hcy5kYXRhLmZyYW1lKGNiaW5kKGdyYWRlLmNvdW50LGdyYWRlLm1lYW4pKVxuXG5vdXRcblxuIyBOb3cgbGV0cyBvcmRlciB0aGUgb3V0IGRhdGEsIGJhc2VkIG9uIHRoZSBncmFkZS5jb3VudCBjb2x1bW4sIGluIGFzY2VuZGluZyBvcmRlci5cbm91dFtvcmRlcihvdXRbLCdncmFkZS5jb3VudCddKSxdXG5cbiMgSWYgeW91IHdhbnQgdGhlIG91dHB1dCBpbiBkZXNjZW5kaW5nIG9yZGVyIGp1c3QgcGFzcyAnVFJVRScgb3IgJ1QnIHRoZSAgZGVjcmVhc2luZyBhcmd1bWVudCBvZiB0aGUgb3JkZXIgZnVuY3Rpb24uXG5vdXRbb3JkZXIob3V0WywnZ3JhZGUuY291bnQnXSxkZWNyZWFzaW5nID0gVCksXSJ9 We saw how we can use implement ordering in R. This is similar to using the ORDER BY statement of SQL. Another thing we can do is subsetting on the output. Suppose you want to keep only those grade records in the out data with frequency of students greater than 150 for particular grade. To do this we will use the technique studied in the slicing section 5.5.2. Lets look at the working of the above example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJtb29keTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvbW9vZHkyMDIwYi5jc3ZcIilcbmdyYWRlLmNvdW50IDwtIHRhcHBseShtb29keSRncmFkZSxtb29keSRncmFkZSxsZW5ndGgpXG5ncmFkZS5tZWFuIDwtIHRhcHBseShtb29keSRzY29yZSxtb29keSRncmFkZSxtZWFuKVxub3V0PC1hcy5kYXRhLmZyYW1lKGNiaW5kKGdyYWRlLmNvdW50LGdyYWRlLm1lYW4pKVxuXG5vdXRcblxuIyBUbyBrZWVwIHRoZSByZWNvcmRzIHdoZXJlIGZyZXF1ZW5jeSBvZiBzdHVkZW50cyBpbiBwYXJ0aWN1bGFyIGdyYWRlIGlzIGdyZWF0ZXIgdGhhbiAxNTAuXG5vdXRbb3V0JGdyYWRlLmNvdW50PjE1MCxdIn0= We see that the B Grade had only 108 students in the record, it is removed from the out dataframe. This is similar to using the HAVING clause of SQL. 5.7 Handling Date and Time in dataframes. one of the most common issue that a novice or even an experienced R user can face is of handling date and time information available into the dataset, and importing it to use as a variable that is appropriate ans usable during analysis. Also getting R to agree that your data contains the dates and times can be tricky sometimes. We will see an example where the usual R data import fails to read date and time as actually date and time. To simplify this issue, we use a package called lubridate, which makes it easier to work with dates and times and converts them into POSIXct format. POSIXct is a class of data recognized by R as being a date or date and time. Lubridates functions handle wide variety of formats and separators, which simplifies the parsing process. Lets look how easy it is to use it and convert date and time input to be used in analysis. First we will look at converting date in character format to POSIXct. First we will convert \"20200317\"which is in year-month-date format. To convert this we will use the ymd() function of the lubridate package. Second we will convert \"03-17-2020\"which is in month-date-year format. To convert this we will use the mdy() function of the lubridate package. Third we will convert \"17/03/2020\"which is in date-month-year format. To convert this we will use the dmy() function of the lubridate package. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KGx1YnJpZGF0ZSkgIyBpbmNsdWRlIHRoZSBsdWJyaWRhdGUgbGlicmFyeS5cblxuIyBGaXJzdCB3ZSB1c2UgdGhlIHltZCgpIGZ1bmN0aW9uLlxueW1kKFwiMjAyMDAzMTdcIilcblxuIyBTZWNvbmQgd2UgdXNlIHRoZSBtZHkoKSBmdW5jdGlvbi5cbm1keShcIjAzLTE3LTIwMjBcIilcblxuIyBUaGlyZCB3ZSB1c2UgdGhlIGRteSgpIGZ1bmN0aW9uLlxuZG15KFwiMTcvMDMvMjAyMFwiKSJ9 We can see the output of all the 3 function is the same, this means that the functions used have successfully converted all the input character type dates into the standardized POSIXct data type. Now lets look at converting time in character format to POSIXct. First we will convert \"18:20\" which is in hour-minutes format. To convert this we ill use the hm() function of the lubridate package. Second we will convert \"18:20:30\" which is in hour-minute-second format. To convert this we ill use the hms() function of the lubridate package. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KGx1YnJpZGF0ZSkgIyBpbmNsdWRlIHRoZSBsdWJyaWRhdGUgbGlicmFyeS5cblxuIyBGaXJzdCB3ZSB3aWxsIHVzZSB0aGUgaG0oKSBmdW5jdGlvbi5cbmhtKFwiMTg6MjBcIilcblxuIyBTZWNvbmQgd2Ugd2lsbCB1c2UgdGhlIGhtcygpIGZ1bmN0aW9uLlxuaG1zKFwiMTg6MjA6MzBcIikifQ== We can see that the output of the 2 functions above are in POSIXct format and has the information of hours minutes and seconds annotated properly. There are various other functions in the lubridate package like for various use case, but we will not cover them since they are not useful here. To learn more about it you can visit the official lubridate package vignette linked here: lubridate Now coming back to the main example of avoiding issues/errors while importing date and time attributes present in dataset. For this we will look at the AirQualityUCI dataset. And here is the snippet of the dataset below. Table 5.4: Air Quality Dataset of amount of elements and pollutants in air. Date Time CO Tin.Oxide Non.Metanic.HydroCarbons Benzene 3/10/2004 18:00:00 2.6 1360 150 11.9 3/10/2004 19:00:00 2.0 1292 112 9.4 3/10/2004 20:00:00 2.2 1402 88 9.0 3/10/2004 21:00:00 2.2 1376 80 9.2 3/10/2004 22:00:00 1.6 1272 51 6.5 3/10/2004 23:00:00 1.2 1197 38 4.7 You will see from the dataset that the date and time columns are imported correctly. But in fact, and as we will see in the code below, the date column is of type character and the time is also of type character. Now to convert these columns into POSIXct supported date time columns we will use the lubridate functions. And then we will count the number of records in the dataset per year using the year() function. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhcTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvQWlyUXVhbGl0eVVDSS5jc3ZcIikgI3dlYiBsb2FkXG5hcTwtYXFbLDE6Nl0gIyBSZWR1Y2luZyB0aGUgbnVtYmVyIG9mIGNvbHVtbnMuXG5cbmhlYWQoYXEpXG5cbiMgTGV0cyBsb29rIGF0IHRoZSB0eXBlIG9mIHRoZSBEYXRlIGNvbHVtbiBhZnRlciBpbXBvcnRpbmcgdGhlIGRhdGFzZXQuXG5jbGFzcyhhcSREYXRlKSAjIERhdGUgQ29sdW1uXG5cbiMgTm93IGxldHMgdXNlIHRoZSBtZHkoKSBmdW5jdGlvbiB3aGljaCBjb252ZXJ0cyB0aGUgbW9udGgtZGF5LXllYXIgZm9ybWF0IHRvIFBPU0lYY3QgZm9ybWF0LlxuYXEkRGF0ZTwtbWR5KGFxJERhdGUpXG5oZWFkKGFxKVxuXG4jIE5vdyBsZXRzIGNoZWNrIHRoZSB0eXBlIG9mIHRoZSBEYXRlIGNvbHVtbiBhZ2Fpbi5cbmNsYXNzKGFxJERhdGUpXG5cblxuIyBMZXRzIGNyZWF0ZSBhIGZyZXF1ZW5jeSB0YWJsZSBmb3IgdGhlIGZyZXF1ZW5jeSBvZiByZWNvcmRzIHBlciB5ZWFyIHVzaW5nIHRoZSB0YWJsZSgpIGFuZCB5ZWFyKCkgZnVuY3Rpb25cbnRhYmxlKHllYXIoYXEkRGF0ZSkpIn0= We can see that the original type of the date column was \"character\" but then after using the lubridates function, we converted it to a suitable POSIXct format of \"Date\". Then we were easily able to subset the dataset based on the year, and get the frequency count of the records per year, as seen from the table for the years 2004 and 2005. Similarly, we can also convert the time column and probably use it later in analysis process. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJhcTwtcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvQWlyUXVhbGl0eVVDSS5jc3ZcIikgI3dlYiBsb2FkXG5hcTwtYXFbLDE6Nl0gIyBSZWR1Y2luZyB0aGUgbnVtYmVyIG9mIGNvbHVtbnMuXG5cbmhlYWQoYXEpXG5cbiMgTGV0cyBsb29rIGF0IHRoZSB0eXBlIG9mIHRoZSBUaW1lIGNvbHVtbiBhZnRlciBpbXBvcnRpbmcgdGhlIGRhdGFzZXQuXG5jbGFzcyhhcSRUaW1lKSAjIFRpbWUgQ29sdW1uXG5cbiMgU2ltaWxhcmx5IGZvciB0aGUgdGltZSBjb2x1bW4gbGV0cyB1c2UgdGhlIGhtcygpIGZ1bmN0aW9uLlxuYXEkVGltZTwtaG1zKGFxJFRpbWUpXG5cbiMgTm93IGxldHMgY2hlY2sgdGhlIHR5cGUgb2YgdGhlIFRpbWUgY29sdW1uIGFnYWluLlxuY2xhc3MoYXEkVGltZSkifQ== We can see that the original type of the time column was \"character\" but then after using the lubridates function, we converted it to a suitable POSIXct format of \"Period\" which is used to represent time information. EOC "],["classification.html", "Chapter 6 Data Modeling and Prediction techniques for Classification. 6.1 Decision Tree. 6.2 Use of Rpart 6.3 Visualize the Decision tree 6.4 Rpart Control 6.5 Prediction using rpart. 6.6 Split the data yourself. 6.7 Cross Validation", " Chapter 6 Data Modeling and Prediction techniques for Classification. 6.1 Decision Tree. Decision tree is one of the most powerful and popular tool for classification and prediction. It is a supervised learning predictive model that uses a set of binary rules to calculate a target value. It is a flow chart like tree structure, where each internal node has a test on a particular attribute, each branch denotes the outcome of the test, and each leaf node holds a class label/ numeric value. The reason decision tree are very popular are: - It is able to generate rules easier to understand as compared to other models.. - It require much less computations for performing modeling and prediction. - Both continuous/numerical and categorical variables are handled easily while creating the decision trees. There are a few drawbacks too while using decision trees in certain case of inputs and tasks. But for the scope of discussion of this course we wont go into much details of it. Also, we wont go into the depth of the internals of the formation/creation of the decision tree and its underlying algorithm, but we will look at decision tree as a way to create prediction model for both classification and regression. 6.2 Use of Rpart Recursive Partitioning and Regression Tree RPART library is a collection of routines which implement Classification and Regression Tree (CART) which is a type of Decision Tree.The resulting model can be represented as a binary tree. The library associated with this RPART is called rpart. Install this library using install.packages(\"rpart\"). Syntax for building the decision tree using rpart(): rpart( formula , method, data, control,...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... method: here we describe the type of decision tree we want. If nothing is provided, the function makes an intelligent guess. We can use anova for regression, class for classification, etc. data: here we provide the dataset on which we want to fit the decision tree on. control: here we provide the control parametes for the decision tree. Explained more in detail in section further in this chapter. For more info on the rpart function visit rpart documentation Lets look at an example on the Moody 2019 dataset. We will use the rpart() function with the following inputs: prediction -&gt; GRADE predictors -&gt; SCORE, ON_SMARTPHONE, ASKS_QUESTIONS, LEAVES_EARLY, LATE_IN_CLASS data -&gt; moody dataset method -&gt; class for classification. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHkgPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTU9PRFktMjAxOS5jc3ZcIilcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24uXG5ycGFydChHUkFERSB+IFNDT1JFK09OX1NNQVJUUEhPTkUrQVNLU19RVUVTVElPTlMrTEVBVkVTX0VBUkxZK0xBVEVfSU5fQ0xBU1MsIGRhdGEgPSBtb29keVssLWMoMSldLG1ldGhvZCA9IFwiY2xhc3NcIikifQ== We can see that the output of the rpart() function is the decision tree with details of, node -&gt; node number split -&gt; split conditions/tests n -&gt; number of records in either branch i.e. subset yval -&gt; output value i.e. the target predicted value. yprob -&gt; probability of obtaining a particular category as the predicted output. Using output tree, we can use the predict function to predict the grades of the test data. We will look at this process later in section 6.5 But coming back to the output of the rpart() function, the text type output is useful but difficult to read and understand, right! We will look at visualizing the decision tree in the next section. 6.3 Visualize the Decision tree To visualize and understand the rpart() tree output in the easiest way possible, we use a library called rpart.plot. The function rpart.plot() of the rpart.plot library is the function used to visualize decision trees. The rpart.plot library is a front-end wrapper to the library prp which is the most basic library for plotting decision trees. prp allows various aesthetic modifications for visualizing the decision tree. We will look at a few examples of using prp below. But, first lets look at a example to visualize the output decision tree in the previous example on Moody dataset using rpart.plot() NOTE: The online runnable code block does not support rpart.plot and prp library and functions, thus the output of the following code examples are provided directly. # First lets import the rpart library library(rpart) # Import dataset moody &lt;- read.csv(&quot;https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv&quot;) # Use of the rpart() function. tree &lt;- rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody,method = &quot;class&quot;) # Now lets import the rpart.plot library to use the rpart.plot() function. library(rpart.plot) # Use of the rpart.plot() function to visualize the decision tree. rpart.plot(tree) Output Plot of rpart.plot() function We can see that after plotting the tree using rpart.plot() function, the tree is more readable and provides better information about the splitting conditions, and the probability of outcomes. Each leaf node has information about the grade category. the outcome probability of each grade category. the records percentage out of total records. To study more in detail the arguments that can be passed to the rpart.plot() function, please look at these guides rpart.plot and Plotting with rpart.plot (PDF) Note that for any beginner using rpart.plot() function is the easiest way. But if you want to learn another way of plotting rpart trees then the following function can be used. So,another form of plotting rpart trees in a very minimalistic way is using the plot rpart i.e. prp() function, which is actually the working function behind rpart.plot(). Lets look at a same example like above but using prp(). # First lets import the rpart library library(rpart) # Import dataset moody &lt;- read.csv(&quot;https://raw.githubusercontent.com/deeplokhande/data101demobook/main/files/dataset/MOODY-2019.csv&quot;) # Use of the rpart() function. tree &lt;- rpart(GRADE ~ SCORE+ON_SMARTPHONE+ASKS_QUESTIONS+LEAVES_EARLY+LATE_IN_CLASS, data = moody[,-c(1)],method = &quot;class&quot;) # Now lets import the rpart.plot library to use the rpart.plot() function. library(rpart.plot) # Use of the prp function to visualize the decision tree. prp(tree) Output Plot of prp() function We can see that the output of the prp() function is a very minimalist tree, without any colors with minimum required information. There are other arguments that can be passed to the prp() function to increase the aesthetic look and the information provided. To learn those extra arguments visit this guide prp() NOTE: In this chapter, from this point forward, the rpart.plots() generated in any example below will be shown as images, and also the code to generate those rpart.plots will be commented in the interactive code blocks. If you want to generate these plots yourself, please use a local Rstudio or R environment. 6.4 Rpart Control We will now look at the control argument used in rpart() function, which is one of the important argument. The control argument of rpart() function is used to manually decide the control parameters of the decision tree. The advantages of using control method: - It restricts the height of the decision tree. - It avoids overfitting on the training dataset. - It can be used to eliminate attributes that affect less significantly on the splitting constraints. - Helps to terminate the creation process of tree earlier, thus reducing required computational time. The disadvantages of using control method: - It creates risk of generating trees with lesser accuracy compared to uncontrolled tree. - It could hamper the splitting condition selection in negative way. - Could result in underfitting, if control parameters not chosen carefully. As we can see, that controlling the decision tree provides us with lot of advantage in certain condition, we also risk in reducing the accuracy of the prediction from using the tree. Thus these control methods must be applied only in certain case, where the uncontrolled method takes large amount of time to create the tree, and overfits the train data. When the datasets have more significantly higher count of columns but less records of data, using control methods could be suitable. Now lets look at the rpart.control() function used to pass the control parameters to the control argument of the rpart() function. rpart.control( *minsplit*, *minbucket*, *cp*,...) minsplit: the minimum number of observations that must exist in a node in order for a split to be attempted. For example, minsplit=500 -&gt; the minimum number of observations in a node must be 500 or up, in order to perform the split at the testing condition. minbucket: minimum number of observations in any terminal(leaf) node. For example, minbucket=500 -&gt; the minimum number of observation in the terminal/leaf node of the trees must be 500 or above. cp: complexity parameter. Using this informs the program that any split which does not increase the accuracy of the fit by cp, will not be made in the tree. For more information of the other arguments of the rpart.control() function visit rpart.control Note: The ratio of minsplt to minbucket is 3:1. Thus if only one of the minsplit/minbucket is provided the other value is set using the above ratio. Also if both values are provided, unless the values are not in the above ratio, the rpart.control() the resorts to the default value. Also note, the default value of cp is 0.01. Let look at few examples. Suppose you want to set the control parameter minsplit=200. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIGxpYnJhcnkocnBhcnQpXG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbiB3aXRoIHRoZSBjb250cm9sIHBhcmFtZXRlciBtaW5zcGxpdD0yMDBcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiAuLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIsY29udHJvbD1ycGFydC5jb250cm9sKG1pbnNwbGl0ID0gMjAwKSlcblxuIyBDaGVjayB0aGUgY291bnQgb2Ygb2JzZXJ2YXRpb24gYXQgZWFjaCBzcGxpdCB0ZXN0LiBUbyBkbyB0aGlzIHdlIGZpbmQgdGhlIGNvdW50IGF0IGVhY2ggbm9uLWxlYWYvbm9uLXRlcm1pbmFsIG5vZGUuXG50cmVlJGZyYW1lW3RyZWUkZnJhbWUkdmFyIT1cIjxsZWFmPlwiLGMoXCJ2YXJcIixcIm5cIildXG5cbiMgbGlicmFyeShycGFydC5wbG90KVxuIyBycGFydC5wbG90KHRyZWUsZXh0cmEgPSAyKSJ9 Output tree plot of after setting minsplit=200 in rpart.control() function We can see from the output of tree$splits and the tree plot, that at each split the total amount of observations are above 200. Also, in comparison to the tree without control, the tree with control has lower height, and lesser count of splits. Now, lets set the minbucket parameter to 100, and see how that affects the tree parameters. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHkgPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTU9PRFktMjAxOS5jc3ZcIilcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24gd2l0aCB0aGUgY29udHJvbCBwYXJhbWV0ZXIgbWluc3BsaXQ9MjAwXG50cmVlIDwtIHJwYXJ0KEdSQURFIH4gLiwgZGF0YSA9IG1vb2R5WywtYygxKV0sbWV0aG9kID0gXCJjbGFzc1wiLGNvbnRyb2w9cnBhcnQuY29udHJvbChtaW5idWNrZXQgPSAxMDApKVxuXG4jIENoZWNrIHRoZSBjb3VudCBvZiBvYnNlcnZhdGlvbiBpbiBlYWNoIGxlYWYgbm9kZS5cbnRyZWUkZnJhbWVbdHJlZSRmcmFtZSR2YXI9PVwiPGxlYWY+XCIsYyhcInZhclwiLFwiblwiKV1cblxuIyBsaWJyYXJ5KHJwYXJ0LnBsb3QpXG4jIHJwYXJ0LnBsb3QodHJlZSxleHRyYSA9IDIpIn0= Output tree plot of after setting minbucket=100 in rpart.control() function We can see for the output and the tree plot, that the count of observations in each leaf node is greater than 100. Also, the tree height has shortened, suggesting that the control method was able to shorten the tree size. Lets now use the cp parameter and see its effect on the tree. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxubW9vZHkgPC0gcmVhZC5jc3YoXCJodHRwczovL3Jhdy5naXRodWJ1c2VyY29udGVudC5jb20vZGVlcGxva2hhbmRlL2RhdGExMDFkZW1vYm9vay9tYWluL2ZpbGVzL2RhdGFzZXQvTU9PRFktMjAxOS5jc3ZcIilcblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24gd2l0aCB0aGUgY29udHJvbCBwYXJhbWV0ZXIgY3A9MC4wMDVcbnRyZWUgPC0gcnBhcnQoR1JBREUgfiAuLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIsY29udHJvbD1ycGFydC5jb250cm9sKGNwID0gMC4wMDUpKVxuXG4jIENoZWNrIHRoZSBhY2N1cmFjeSBpbmNyZWFzZSBmYWN0b3IgYXQgZWFjaCBzcGxpdC5cbnRyZWUkY3B0YWJsZVxuXG4jIGxpYnJhcnkocnBhcnQucGxvdCkpXG4jIHJwYXJ0LnBsb3QodHJlZSkifQ== Output tree plot of after setting cp=0.005 in rpart.control() function We can see for the output and the tree plot, that the tree size has increased, with increase in number of splits, and leaf nodes. Also we can see that the minimum CP value in the output is 0.005. Now we saw the most important control parameters of the rpart.control function. Remember there are other parameters too, which you can study if you wish, but studying just these 3 discussed above are sufficient for the scope of this course. Also, note we have not check the effects of the control parameters on the prediction accuracy of the decision tree. Using the control parameters you could either increase the accuracy, but also risk, decreasing the accuracy. So choosing the controls parameter very carefully is very important to push the accuracy in the right direction. 6.5 Prediction using rpart. Now that we saw process to create a decision tree and also plotting it, we will like to use the output tree to predict the required attribute. From the moody example, we are trying to predict the grade of students. Lets look at the predict() function to predict the outcomes. predict(*object*,*data*,*type*,...) object: the generated tree from the rpart function. data: the data on which the prediction is to be performed. type: the type of prediction required. One of vector, prob, class or matrix. Now lets use the predict function to predict the grades of students using the tree generated on the Moody dataset. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbi5cbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStPTl9TTUFSVFBIT05FK0FTS1NfUVVFU1RJT05TK0xFQVZFU19FQVJMWStMQVRFX0lOX0NMQVNTLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIpXG5cbiMgTm93IGxldHMgcHJlZGljdCB0aGUgR3JhZGVzIG9mIHRoZSBNb29keSBEYXRhc2V0LlxucHJlZCA8LSBwcmVkaWN0KHRyZWUsIG1vb2R5LCB0eXBlPVwiY2xhc3NcIilcbmhlYWQocHJlZCkifQ== We see that the output of the predict function is a vector of grades corresponding to each record of the Moody dataframe. Each index has a grade among A, B, C, D, F. Although we see the output, how do we compare the accuracy and correctness of the outputs. Lets look at one of the basic test we can do is perform a record by record comparison of the grade already in the dataset and the predicted grade, in the example below. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEZpcnN0IGxldHMgaW1wb3J0IHRoZSBycGFydCBsaWJyYXJ5XG5saWJyYXJ5KHJwYXJ0KVxuXG4jIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFVzZSBvZiB0aGUgcnBhcnQoKSBmdW5jdGlvbi5cbnRyZWUgPC0gcnBhcnQoR1JBREUgfiBTQ09SRStPTl9TTUFSVFBIT05FK0FTS1NfUVVFU1RJT05TK0xFQVZFU19FQVJMWStMQVRFX0lOX0NMQVNTLCBkYXRhID0gbW9vZHlbLC1jKDEpXSxtZXRob2QgPSBcImNsYXNzXCIpXG5cbiMgTm93IGxldHMgcHJlZGljdCB0aGUgR3JhZGVzIG9mIHRoZSBNb29keSBEYXRhc2V0LlxucHJlZCA8LSBwcmVkaWN0KHRyZWUsIG1vb2R5LCB0eXBlPVwiY2xhc3NcIilcbmhlYWQocHJlZClcblxuIyBMZXRzIGNoZWNrIHRoZSBjb3JyZWN0bmVzcyBvZiBlYWNoIHByZWRpY3Rpb24gZm9yIGVhY2ggcmVjb3JkXG5tZWFuKG1vb2R5JEdSQURFPT1wcmVkKSoxMDAifQ== Using just a row by row comparison we can see that the outcomes of the predicted grades and the original grades from the Moody dataset, are matching 93.73%. Thus our prediction accuracy is 93.73% and the error rate is 6.27%, which is very good. This prediction accuracy calculated on the training dataset is called training accuracy. Notice that we just compared the predicted grades with the already present grades from the Moody dataset, the same dataset on which the tree was built. In such scenario, one can say that you are predicting the grades on data already considered while training. So in some sense, you just output the known grades, and did not do any useful prediction. But to really see the use of our tree, we must predict on a data which has never been used in the training of the tree, OR, where the Prof. Moody has not assigned the grades. In the latter case it is difficult to prove the accuracy, without actually checking with Prof. Moody, if he/she would have assigned the same grade as the grade predicted by our model. But we have a solution for the former case, where we can predict grades on a subset of data, which we have not used while training. For that we would need to split the provided data into 2 parts, Training and Testing and then repeat the training process on the training dataset and the prediction on testing dataset. 6.6 Split the data yourself. We introduced the first and basic model for data analysis, Decision tree, in the section above. But we found that we need to perform a basic routine before dwelling deep into the creation of decision tree. The routine is to split the dataset in multiple parts, to check the accuracy of our trees prediction. This routine is the first step you perform after acquiring a cleaned dataset. The most useful splitting of dataset is done in 2 parts, Training and Testing. While splitting, the split ratio between training and testing should be decided properly. Mostly, training data is kept bigger,and testing is done on a relatively smaller subset. But the ratio should not be too biased, where there are only few observations in test data compared to training data. Usually, the math behind splitting is that, even after split, the smaller subset, i.e. the test subset should represent the distribution of the complete dataset. This means the test data should at-least have few record of each possible combination of attributes categories if categorical data, or, if numerical data then the numerical distribution is same as of the complete dataset. Thus, typically the train-test ratio is 80-20 or 70-30 or in some case even 60-40. Also, while selecting the records to assign to either training/testing data, they should be randomly picked from the original data, so as to avoid unbalanced distribution. We will look at a small example of splitting the complete dataset into training and testing dataset with a 70-30 ratio. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFNwbGl0IHJhbmRvbWx5IGludG8gMiBzZXRzIHdpdGggY2VydGFpbiByYXRpby9wcm9iYWJpbGl0eS5cbmlkeCA8LSBzYW1wbGUoIDE6Miwgc2l6ZSA9IG5yb3cobW9vZHkpLCByZXBsYWNlID0gVFJVRSwgcHJvYiA9IGMoLjcsIC4zKSlcbm1vb2R5LnRyYWluIDwtIG1vb2R5W2lkeCA9PSAxLF1cbm1vb2R5LnRlc3QgPC0gbW9vZHlbaWR4ID09IDIsXVxuXG5ucm93KG1vb2R5KVxubnJvdyhtb29keS50cmFpbilcbm5yb3cobW9vZHkudHJhaW4pL25yb3cobW9vZHkpXG5ucm93KG1vb2R5LnRlc3QpXG5ucm93KG1vb2R5LnRlc3QpL25yb3cobW9vZHkpIn0= As we can see we split the original data with 1580 rows into two dataset, training data with almost 70% of rows of the original, and testing data with almost 30% of the original. Notice that we used a random sampling of the data, and not just sequential, to avoid any unbalanced distribution of attributes. Now, we looked at a method to split the dataset into training and testing data. But there is another type of splitting of the dataset which involves splitting the data into 3 parts namely, training, cross-validation and testing. We will look at the use of cross-validation and the process, in the next section 6.7. Typically, the ratio of train-validation-test is 60-20-20 or 50-25-25. Before that lets look at a simple method to perform a 3 way split with ratio 60-20-20. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIEltcG9ydCBkYXRhc2V0XG5tb29keSA8LSByZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9NT09EWS0yMDE5LmNzdlwiKVxuXG4jIFNwbGl0IHJhbmRvbWx5IGludG8gMyBzZXRzIHdpdGggY2VydGFpbiByYXRpby9wcm9iYWJpbGl0eS5cbmlkeCA8LSBzYW1wbGUoIDE6Mywgc2l6ZSA9IG5yb3cobW9vZHkpLCByZXBsYWNlID0gVFJVRSwgcHJvYiA9IGMoMC42LCAwLjIsIDAuMikpXG5tb29keS50cmFpbiA8LSBtb29keVtpZHggPT0gMSxdXG5tb29keS52YWxpZGF0aW9uIDwtIG1vb2R5W2lkeCA9PSAyLF1cbm1vb2R5LnRlc3QgPC0gbW9vZHlbaWR4ID09IDMsXVxuXG5ucm93KG1vb2R5KVxubnJvdyhtb29keS50cmFpbilcbm5yb3cobW9vZHkudHJhaW4pL25yb3cobW9vZHkpXG5ucm93KG1vb2R5LnZhbGlkYXRpb24pXG5ucm93KG1vb2R5LnZhbGlkYXRpb24pL25yb3cobW9vZHkpXG5ucm93KG1vb2R5LnRlc3QpXG5ucm93KG1vb2R5LnRlc3QpL25yb3cobW9vZHkpIn0= We can see that the dataset is split into 3 parts, with 60% in training data, 20% in validation data, and 20% in testing data. 6.7 Cross Validation Cross validation is a model validation technique for assessing generalization of the results of statistical analysis to an independent dataset. In other words, it is a technique to estimate the accuracy of a predictive models performance in practice. The goal of cross-validation is to test the models ability to predict new data that was not used in estimating/training it, in order to avoid problems like over-fitting and selection bias, and to give an insight on how the model will generalize to an independent dataset(i.e., an unknown dataset). Cross-validation also helps in selecting and fine-tuning the hyper-parameters of the models. In our case of decision tree, the hyper parameters could be the control parameters that determind the size of the decision tree, which in-turn determines the accuracy of the tree. One round of cross-validation involves partitioning data into complementary subsets, and then performing model training on one subset, and validating the results on the other subset. In most methods, multiple rounds of cross-validation are performed using different partitions in each round, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the models predictive performance. Another use of cross-validation is when you dont have the test data, and hence, you dont have a way to determine the true accuracy of the model. Because we cannot determine accuracy on test dataset, we partition our training dataset into train and validation (testing). We train our model (rpart or lm) on train partition and test on the validation partition. The accuracy on the validation data is called cross-validation accuracy, while that on the train data is called training accuracy. Lets not dive too deep into the theory of this cross validation technique, but lets learn about the cross_validate() function, that helps us achieve this. cross_validate(*data*, *tree*, *n_iter*, *split_ratio*, *method*) data: The dataset on which cross validation is to be performed. tree: The decision tree generated using rpart. n_iter: Number of iterations. split_ratio: The splitting ratio of the data into train data and validation data. method: Method of the prediction. class for classification. The way the function works is as follows: It randomly partitions your data into training and validation. It then constructs the following two decision trees on training partition: The tree that you pass to the function. The tree constructed on all attributes as predictors and with no control parameters. -It then determines the accuracy of the two trees on validation partition and returns you the accuracy values for both the trees. The first column corresponds to the cross-validation accuracy on the tree that you pass; the second is the cross-validation accuracy on the tree without any control and all attributes. The values in the first column(accuracy_subset) returned by cross-validation function are more important when it comes to detecting overfitting. If these values are much lower than the training accuracy you get, that means you are overfitting. We would also want the values in accuracy_subset to be close to each other (in other words, have low variance). If the values are quite different from each other, that means your model (or tree) has a high variance which is not desired. The second column(accuracy_all) tells you what happens if you construct a tree based on all attributes. If these values are larger than accuracy_subset, that means you are probably leaving out attributes from your tree that are relevant. Each iteration of cross-validation creates a different random partition of train and validation, and so you have possibly different accuracy values for every iteration. Lets look at the cross_validate() function in action in the example below. We will pass the tree with formula as GRADE ~ SCORE+ON_SMARTPHONE+LEAVES_EARLY, and control parameter, with minsplit=100. And for cross_validate() function, we will usen_iter=5, and split_raitio=0.7 NOTE: Cross-Validation repository is already preloaded for the following interactive code block. Thus you can directly use the cross_validate() function in the following interactive code block. But if you wish to use the code_validate() function locally, please use install.packages(&quot;devtools&quot;) devtools::install_github(&quot;devanshagr/CrossValidation&quot;) CrossValidation::cross_validate() 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 NOTE: If you encounter error while running the cross-validation function that said new levels encountered in test, make sure the dataset is imported again with read.csv() attribute stringsAsFactors as TRUE or T. For more information about the inner-working of the cross_validate() function visit cross_validate() We can see in the output the Training accuracy, the table of cross-validation accuracy at each iteration for both the passed tree and the tree on all attribute and also their averages and variances. Few Observation from the selected example above are: For the tree passed with selected attributes and some control parameters, the cross-validation accuracys (i.e. accuracy values in the accuracy_subset column) are fairly high for all iterations and have very low variance. They are close to the training accuracy which indicates we are not overfitting. Observe that the accuracy at each iteration of the accuracy_subset and accuracy_all column are relatively, close but not exact, suggesting that there are more attributes or other control parameters that can be included to the passed tree, to further increase the accuracy, thus closing the gap. Thus using cross-validation we were able to figure out with certainty, that the passed tree, is not the best tree that can be created using the training data. Also, we saw whether the generated tree overfits the training data or not. EOC "],["pred1blog.html", "Chapter 7 Blog: Prediction Challenge 1.* 7.1 The prediction challange.* 7.2 Top Submissions and their experience.*", " Chapter 7 Blog: Prediction Challenge 1.* 7.1 The prediction challange.* 7.2 Top Submissions and their experience.* "],["regression.html", "Chapter 8 Data Modelling and Prediction techniques for Regression. 8.1 Linear Regression. 8.2 Regression using RPART", " Chapter 8 Data Modelling and Prediction techniques for Regression. In chapter 6 we studied modeling and prediction techniques for Classification, where we predicted the class of the output variable, using Decision tree based algorithms. In this chapter we will study techniques for regression based prediction and create models for it. As opposite the the classification where we predict a particular class of the output variable, here we will predict a numerical/continuous value. We will look at two methods of performing this regression based modeling and prediction, first simple linear regression and second regression using decision tree. 8.1 Linear Regression. Linear regression is a linear approach to modeling the relationship between a scalar response and one or more explanatory variables(also known as dependent and independent variables.) Scalar response means the predicted output. Usually in linear regression, models are used to predict only one scalar variable. This type of linear regression model known as simple linear regression. But in the case of prediction of multiple correlated output variables, this type of prediction using linear regression model is called multivariate linear regression. Explanatory variables are the predictors on which the output predictions are based on. In the regression setting there can be two type of explanatory variables, The dependent variables i.e. they are derived by applying some law/rule or function onto some other variable/s The independent variables which are independently sufficient to used are predictors while use in regression. Since the relationship between the explanatory variables and the output variable is modeled linearly, these models are called as linear models. To do this, we need to find a linear regression equation for the set of input predictors and the output variable. But without going into the mathematics of finding this linear regression equation, we will use a tool/function provided in R to model and predict the output variable. 8.1.1 Linear regression using lm() function Syntax for building the regression model using the lm() function is as follows: lm(formula, data, ...) formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... data: here we provide the dataset on which the linear regression model is to be trained. For more info on the lm() function visit lm() Lets look at the example on the RealEstate dataset. A snippet of the Realestate Dataset is given below. Table 8.1: Snippet of Real Estate Dataset Price Bedrooms Bathrooms Size 795000 3 3 2371 399000 4 3 2818 545000 4 3 3032 909000 4 4 3540 109900 3 1 1249 324900 3 3 1800 192900 4 2 1603 215000 3 2 1450 999000 4 3 3360 319000 3 2 1323 Now we can build a simple linear regression model to predict the Price attribute based on the various other attributes present in the dataset, as shown above. Since we will be predicting only one attribute values, this model will be called simple linear regression model. For the first example we will predict the price value of house using only size attribute as the predictor. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXQuXG5yZWFsZXN0YXRlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9SZWFsRXN0YXRlLmNzdlwiKVxuXG50cmFpbiA8LSByZWFsZXN0YXRlWzE6NjAwLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVs2MDE6NzgxLF1cblxuIyBVc2UgdGhlIGxtKCkgZnVuY3Rpb24gdG8gcHJlZGljdCB0aGUgcHJpY2UgYmFzZWQgb24gc2l6ZSBvZiB0aGUgaG91c2UuXG4jIFRodXMgdGhpcyBpcyBhbiBleGFtcGxlIG9mIHNpbXBsZSBsaW5lYXIgcmVncmVzc2lvbiBzaW5jZSBvbmx5IG9uZSBwcmVkaWN0b3IgYW5kIG9uZSBvdXRwdXQgdmFsdWUgaXMgdXNlZC5cbnNpbXBsZS5maXQgPC0gbG0oUHJpY2V+U2l6ZSxkYXRhPXRyYWluKVxuXG4jIHN1bW1hcnkgb2YgdGhlIG1vZGVsXG5zdW1tYXJ5KHNpbXBsZS5maXQpXG5cbiMgTGluZWFyIHJlbGF0aW9uIGJldHdlZW4gdGhlIFByaWNlIGFuZCBTaXplIGF0dHJpYnV0ZS5cbnBsb3QodHJhaW4kU2l6ZSx0cmFpbiRQcmljZSlcbmFibGluZShzaW1wbGUuZml0ICwgY29sPVwicmVkXCIpXG5cbiMgUHJlZGljdGluZyB2YWx1ZXMgb24gdGhlIHRlc3QgZGF0YXNldC5cblByZWRpY3RlZFByaWNlLnNpbXBsZSA8LSBwcmVkaWN0KHNpbXBsZS5maXQsdGVzdClcbiMgUHJlZGljdGVkIFZhbHVlc1xuaGVhZChhcy5pbnRlZ2VyKHVubmFtZShQcmVkaWN0ZWRQcmljZS5zaW1wbGUpKSlcbiMgQWN0dWFsIFZhbHVlc1xuaGVhZCh0ZXN0JFByaWNlKSJ9 We can see that, The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc. The plot of Size vs Price, and the red line represents the fitted line or the linear model line which will be used for prediction. The predicted values are obtained form the predict function using the trained model and the test data. In comparison to the actual values, the predicted values are some times close (e.g. index 2),some time far(e.g. index 1,4,5) and few are very far(e.g. 3,6). We saw above an example of simple linear regression model, where only one predictor was used for predicting a single output attribute. Now we will see an example of multiple linear regression model, where there can be multiple predictors to predict a single output attribute. (Note: Please do not confuse this with the multivariate linear regression.) Let look at an example of predicting the Price of the real estate, based on 3 attributes Size, Number of Bedrooms and Number of Bathrooms. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXQuXG5yZWFsZXN0YXRlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9SZWFsRXN0YXRlLmNzdlwiKVxuXG50cmFpbiA8LSByZWFsZXN0YXRlWzE6NjAwLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVs2MDE6NzgxLF1cblxuIyBVc2UgdGhlIGxtKCkgZnVuY3Rpb24gdG8gcHJlZGljdCB0aGUgcHJpY2UgYmFzZWQgb24gc2l6ZSwgYmF0aHJvb21zIGFuZCBiZWRyb29tcyBvZiB0aGUgaG91c2UuXG4jIFRodXMgdGhpcyBpcyBhbiBleGFtcGxlIG9mIG11bHRpcGxlIGxpbmVhciByZWdyZXNzaW9uIHNpbmNlIG11bHRpcGxlIHByZWRpY3RvciBhbmQgb25lIG91dHB1dCB2YWx1ZSBpcyB1c2VkLlxubXVsdGlwbGUuZml0IDwtIGxtKFByaWNlflNpemUgKyBCYXRocm9vbXMgKyBCZWRyb29tcyxkYXRhPXRyYWluKVxuXG4jIHN1bW1hcnkgb2YgdGhlIG1vZGVsXG5zdW1tYXJ5KG11bHRpcGxlLmZpdClcblxuIyBQcmVkaWN0aW5nIHZhbHVlcyBvbiB0aGUgdGVzdCBkYXRhc2V0LlxuUHJlZGljdGVkUHJpY2UubXVsdGlwbGUgPC0gcHJlZGljdChtdWx0aXBsZS5maXQsdGVzdClcbiMgUHJlZGljdGVkIFZhbHVlc1xuaGVhZChhcy5pbnRlZ2VyKHVubmFtZShQcmVkaWN0ZWRQcmljZS5tdWx0aXBsZSkpKVxuIyBBY3R1YWwgVmFsdWVzXG5oZWFkKHRlc3QkUHJpY2UpIn0= We can see that, The summary of the lm model give us information about the parameters of the model, the residuals and coefficients, etc. The predicted values are obtained form the predict function using the trained model and the test data. In comparison to the previous model based on just the Size as predictor, here, when we used 3 predictors, we have more accurate predictions, thus increasing the overall accuracy of the model. 8.1.2 Calculating the Error using mse() As was the simple case in the categorical predictions of the classification models, where we could just compare the predicted categories and the actual categories, this type of direct comparison as an accuracy test wont prove useful now in our numerical predictions scenario. Also we dont want to eyeball everytime we predict, to find the accuracy of our predictions each row by row, so lets see a method to calculate the accuracy of our predictions, using some statistical technique. To do this we will use the Mean Squared Error(MSE). The MSE is a measure of the quality of an predictor/estimator It is always non-negative Values closer to zero are better. The equation to calculate the MSE is as follows: \\[\\begin{equation} MSE=\\frac{1}{n} \\sum_{i=1}^{n}{(Y_i - \\hat{Y_i})^2} \\\\ \\text{where $n$ is the number of data points, $Y_i$ are the observed value}\\\\ \\text{and $\\hat{Y_i}$ are the predicted values} \\end{equation}\\] To implement this, we will use the mse() function present in the Metrics Package, so remember to install the Metrics package and use library(Metrics) in the code for local use. The syntax for mse() function is very simple: mse(actual,predicted) actual: vector of the actual values of the attribute we want to predict. predicted: vector of the predicted values obtained using our model. Now lets look at the MSE of the previous example. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiIjIExvYWQgdGhlIGRhdGFzZXQuXG5yZWFsZXN0YXRlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9SZWFsRXN0YXRlLmNzdlwiKVxuXG50cmFpbiA8LSByZWFsZXN0YXRlWzE6NjAwLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVs2MDE6NzgxLF1cblxuIyBVc2UgdGhlIGxtKCkgZnVuY3Rpb24gdG8gcHJlZGljdCB0aGUgcHJpY2UgYmFzZWQgb24gc2l6ZSBvZiB0aGUgaG91c2UuXG5zaW1wbGUuZml0IDwtIGxtKFByaWNlflNpemUsZGF0YT10cmFpbilcbiMgUHJlZGljdGluZyB2YWx1ZXMgb24gdGhlIHRlc3QgZGF0YXNldC5cblByZWRpY3RlZFByaWNlLnNpbXBsZSA8LSBwcmVkaWN0KHNpbXBsZS5maXQsdGVzdClcbiMgUHJlZGljdGVkIFZhbHVlc1xuaGVhZChhcy5pbnRlZ2VyKHVubmFtZShQcmVkaWN0ZWRQcmljZS5zaW1wbGUpKSlcbiMgQWN0dWFsIFZhbHVlc1xuaGVhZCh0ZXN0JFByaWNlKVxuXG4jIExldHMgdXNlIHRoZSBtc2UoKSBmdW5jdGlvbiB0byBcbm1zZSh0ZXN0JFByaWNlLFByZWRpY3RlZFByaWNlLnNpbXBsZSkifQ== We can see the MSE is too large above 200 billion, and this is huge value could be understandable as we are taking the squared differences of all the records that we predicted. The main intention is to get this huge value to as low as possible possibly near zero, which could be difficult but can be achieved upto a relative error by using a better model and training data. 8.2 Regression using RPART Since we have already used the rpart library for performing decision tree algorithms also referred as CART(classification and regression tree) algorithms, we will now look at this type algorithm for regression based prediction. Remember we have discussed the usage of Rpart in the section 6.2 in great detail. Thus for using Rpart for regression based prediction we will need to provide the rpart() functions, method attribute, with the keyword anova. For more details on the use of Rpart for prediction please refer to section 6.2. Lets look at an example of regression based prediction using Rpart for the Price attribute of the Real estate Dataset with Size, Number of Bedrooms and Number of Bathrooms as predictors. eyJsYW5ndWFnZSI6InIiLCJzYW1wbGUiOiJsaWJyYXJ5KHJwYXJ0KVxuXG4jIExvYWQgdGhlIGRhdGFzZXQuXG5yZWFsZXN0YXRlPC1yZWFkLmNzdihcImh0dHBzOi8vcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbS9kZWVwbG9raGFuZGUvZGF0YTEwMWRlbW9ib29rL21haW4vZmlsZXMvZGF0YXNldC9SZWFsRXN0YXRlLmNzdlwiKVxuXG50cmFpbiA8LSByZWFsZXN0YXRlWzE6NjAwLF1cbnRlc3QgPC0gcmVhbGVzdGF0ZVs2MDE6NzgxLF1cblxuIyBVc2Ugb2YgdGhlIHJwYXJ0KCkgZnVuY3Rpb24gdG8gcHJlZGljdCB0aGUgcHJpY2UgYmFzZWQgb24gdGhlIHNpemUsIGJhdGhyb29tcyBhbmQgYmVkcm9vbXMgb2YgdGhlIGhvdXNlLlxucnBhcnQuZml0IDwtIHJwYXJ0KFByaWNlflNpemUrQmF0aHJvb21zK0JlZHJvb21zLGRhdGE9dHJhaW4sbWV0aG9kID0gXCJhbm92YVwiKVxuXG4jIFByZWRpY3RpbmcgdmFsdWVzIG9uIHRoZSB0ZXN0IGRhdGFzZXQuXG5QcmVkaWN0ZWRQcmljZS5ycGFydCA8LSBwcmVkaWN0KHJwYXJ0LmZpdCx0ZXN0KVxuIyBQcmVkaWN0ZWQgVmFsdWVzXG5oZWFkKGFzLmludGVnZXIodW5uYW1lKFByZWRpY3RlZFByaWNlLnJwYXJ0KSkpXG4jIEFjdHVhbCBWYWx1ZXNcbmhlYWQodGVzdCRQcmljZSlcblxuIyBMZXRzIHVzZSB0aGUgbXNlKCkgZnVuY3Rpb24gdG8gXG5tc2UodGVzdCRQcmljZSxQcmVkaWN0ZWRQcmljZS5ycGFydCkifQ== Output tree plot of rpart() model using for regression using anova method We can see, The output decision tree of the rpart() function The predicted values obtained using the model created by the rpart() function. The MSE of the model on the testing dataset. An important point to note while using decision trees for regression purpose, is that since the underlying process of modeling is still a decision tree, the output still represent a set of distinct classes, even though the values of the classes are numeric. Thus we can see that the predicted values are repeated even for varying inputs. Hence Decision tree must be used carefully when used for regression based prediction models. EOC "],["models.html", "Chapter 9 Additional Modeling techniques.* 9.1 Four Line Method for creating most type of prediction models in R 9.2 Random Forest 9.3 SVM* 9.4 nnet*", " Chapter 9 Additional Modeling techniques.* In this chapter, we will see some additional machine learning models used in practice, for various purpose. After studying both classification models and regression models in the previous 2 chapters 6 &amp; 8 respectively, we will now look into other generic models used for classification and/or regression purpose. Below is the list some of the widely used algorithms with their use case(either classification/regression or both) and training and prediction complexities for using particular learning models. Usage and Complexity of various machine learning algorithms. Credits:thekerneltrip.com As we can see that many of these algorithms can be used for classification and regression all together, as we saw in the case of the Decision tree models using Rpart in section 6.1, and also some model used for only a particular type of prediction e.g. linear regression. We will look at few algorithms from the above list: Random Forest Support Vector Machine (SVM) Neural Metworks and also Linear Discriminant Analysis (LDA) 9.1 Four Line Method for creating most type of prediction models in R But before we learn about these algorithms, let us see a four line method to build models using any of the above algorithms using R. We can safely assume that the data going to be used to build the model, has been pre-processed and based on requirements split into the required subsets. To see how to split the data refer to section 6.6. The zeroth step now will be obviously to install and load the packages that contain the ML algorithm. To do that on your local machine, use the following code. # Install the library install.packages(&quot;package name&quot;) # Load the library in R library(package_name) Once we have the algorithm library loaded, we then proceed to build the model. pred.model = model_function(formula, data, method*) model_function(): the function present in the library to build the model. e.g. rpart() formula: here we mention the prediction column and the other related columns(predictors) on which the prediction will be based on. prediction ~ predictor1 + predictor2 + predictor3 + ... data: here we provide the dataset on which the ML model is to be trained on. Remember never used the test data to build the model. method: (OPTIONAL) Used to denote the method of prediction or underlying algorithm. This parameter could be present in some model_function() but not all. Prediction using the predict() function on the training data to assess the models performance/accuracy in next step. pred = predict(pred.model, newdata = train) predict(): the common function for all models used for prediction. pred.model: output of the step 1. newdata: here we assign the data on which the prediction is to be done. Evaluate error in Training phase. We use the mse() function for finding the accuracy of the model. To read more in dept about the mse() function refer to section 8.1.2. mse(actual, pred) actual: vector of the actual values of the attribute we want to predict. pred: vector of the predicted values obtained using our model. Repeat steps 2 and 3 by changing the ML algorithm or manipulating dataset to perform better when used to train using ML model, so as to achieve as low MSE value as possible. Finally we predict on the testing data using the same predict function as in step 2 but replacing the train data with test data. pred = predict(pred.model, newdata = test) These are the 4 steps to follow while performing any prediction task using ML models in R. We can also add one more step between step 3 and 4, which is step of performing the cross validation process on the newly built models. This can be done either manually, or by using third party libraries. One such library is the rModeling package, which has function crossValidation() which can be used for any type of model_functions(). For more information visit crossValidation() Now that we saw the general structure of the model lets look at few of the algorithms as we promised from the list above. 9.2 Random Forest 9.3 SVM* 9.4 nnet* "]]
